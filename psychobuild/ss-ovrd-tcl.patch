--- linux-2.6.24-palm/drivers/cpufreq/Kconfig	2010-06-11 17:48:34.000000000 -0400
+++ linux-2.6.24-ss/drivers/cpufreq/Kconfig	2010-12-23 09:26:35.000000000 -0500
@@ -53,6 +53,23 @@
 
 	  If in doubt, say N.
 
+config CPU_FREQ_OVERRIDE
+	bool "Extra on-demand CPU tweaking options"
+	default y
+	help
+	  This will give options to tweak CPU settings in-demand.
+
+	  If in doubt, say Y.
+
+config CPU_FREQ_OVERRIDE_STRIPOPP
+	bool "Strip OPP1 and OPP2 from available frequencies list"
+	depends on CPU_FREQ_OVERRIDE
+	default y
+	help
+	  This will hide 125MHz and 250MHz from scaling_available_frequencies.
+
+	  If in doubt, say N.
+
 choice
 	prompt "Default CPUFreq governor"
 	default CPU_FREQ_DEFAULT_GOV_USERSPACE if CPU_FREQ_SA1100 || CPU_FREQ_SA1110
@@ -87,6 +104,22 @@
 	  program shall be able to set the CPU dynamically without having
 	  to enable the userspace governor manually.
 
+config CPU_FREQ_DEFAULT_GOV_SCREENSTATE
+        bool "screenstate"
+        select CPU_FREQ_GOV_SCREENSTATE
+        help
+          Use the CPUFreq governor 'screenstate' as default. This will
+	  scale the CPU frequency down when the LCD is off then scale
+	  back to max speed when LCD is powered on.  This also will not
+	  allow to set the CPU frequency manually.
+
+config CPU_FREQ_DEFAULT_GOV_VDEMAND
+	bool "vdemand"
+	select CPU_FREQ_GOV_VDEMAND
+	help
+	  Use the CPUFreq governor 'vdemand' as default. This will
+	  scale the CPU up and down depending on load.
+
 config CPU_FREQ_DEFAULT_GOV_ONDEMAND
 	bool "ondemand"
 	select CPU_FREQ_GOV_ONDEMAND
@@ -99,6 +132,19 @@
 	  governor. If unsure have a look at the help section of the
 	  driver. Fallback governor will be the performance governor.
 
+config CPU_FREQ_DEFAULT_GOV_ONDEMAND_TICKLE
+	bool "ondemandtcl"
+	select CPU_FREQ_GOV_ONDEMAND_TICKLE
+	select CPU_FREQ_GOV_ONDEMAND
+	select CPU_FREQ_GOV_PERFORMANCE
+	help
+	  Use the CPUFreq governor 'ondemandtcl' as default. This allows
+	  you to get a full dynamic frequency capable system by simply
+	  loading your cpufreq low-level hardware driver, plus tickling.
+	  Be aware that not all cpufreq drivers support the ondemand
+	  governor. If unsure have a look at the help section of the
+	  driver. Fallback governor will be the performance governor.
+
 config CPU_FREQ_DEFAULT_GOV_CONSERVATIVE
 	bool "conservative"
 	select CPU_FREQ_GOV_CONSERVATIVE
@@ -149,6 +195,27 @@
 
 	  If in doubt, say Y.
 
+config CPU_FREQ_GOV_SCREENSTATE
+        tristate "'screenstate' governor for frequency scaling"
+        help
+          Enable this cpufreq governor to scale when LCD is on/off.
+
+          To compile this driver as a module, choose M here: the
+          module will be called cpufreq_screenstate.
+
+          If in doubt, say Y.
+
+config CPU_FREQ_GOV_VDEMAND
+	tristate "'vdemand' governor for frequency scaling"
+	help
+	  Enable this cpufreq governor to scale CPU voltage.
+
+	  To compile this driver as a module, choose M here: the
+	  module will be called cpufreq_vdemand.
+
+	  If in doubt, say Y.
+
+
 config CPU_FREQ_GOV_ONDEMAND
 	tristate "'ondemand' cpufreq policy governor"
 	select CPU_FREQ_TABLE
@@ -167,6 +234,24 @@
 
 	  If in doubt, say N.
 
+config CPU_FREQ_GOV_ONDEMAND_TICKLE
+	tristate "'ondemandtcl' cpufreq policy governor"
+	select CPU_FREQ_TABLE
+	help
+	  'ondemand' - This driver adds a dynamic cpufreq policy governor.
+	  The governor does a periodic polling and
+	  changes frequency based on the CPU utilization, plus tickling.
+	  The support for this governor depends on CPU capability to
+	  do fast frequency switching (i.e, very low latency frequency
+	  transitions).
+
+	  To compile this driver as a module, choose M here: the
+	  module will be called cpufreq_ondemand.
+
+	  For details, take a look at linux/Documentation/cpu-freq.
+
+	  If in doubt, say N.
+
 config CPU_FREQ_GOV_CONSERVATIVE
 	tristate "'conservative' cpufreq governor"
 	depends on CPU_FREQ
--- linux-2.6.24-palm/drivers/cpufreq/Makefile	2008-01-24 17:58:37.000000000 -0500
+++ linux-2.6.24-ss/drivers/cpufreq/Makefile	2010-12-23 09:44:27.000000000 -0500
@@ -8,8 +8,14 @@
 obj-$(CONFIG_CPU_FREQ_GOV_POWERSAVE)	+= cpufreq_powersave.o
 obj-$(CONFIG_CPU_FREQ_GOV_USERSPACE)	+= cpufreq_userspace.o
 obj-$(CONFIG_CPU_FREQ_GOV_ONDEMAND)	+= cpufreq_ondemand.o
+obj-$(CONFIG_CPU_FREQ_GOV_ONDEMAND_TICKLE)	+= cpufreq_ondemand_tickle.o
 obj-$(CONFIG_CPU_FREQ_GOV_CONSERVATIVE)	+= cpufreq_conservative.o
+obj-$(CONFIG_CPU_FREQ_GOV_SCREENSTATE)	+= cpufreq_screenstate.o
+# obj-$(CONFIG_CPU_FREQ_GOV_VDEMAND)	+= cpufreq_vdemand.o
 
 # CPUfreq cross-arch helpers
 obj-$(CONFIG_CPU_FREQ_TABLE)		+= freq_table.o
 
+# CPUfreq override
+obj-$(CONFIG_CPU_FREQ_OVERRIDE)         += cpufreq_override.o
+
--- linux-2.6.24-palm/drivers/cpufreq/cpufreq.c	2010-06-11 17:48:34.000000000 -0400
+++ linux-2.6.24-ss/drivers/cpufreq/cpufreq.c	2010-09-02 13:10:13.000000000 -0400
@@ -32,6 +32,11 @@
 #define dprintk(msg...) cpufreq_debug_printk(CPUFREQ_DEBUG_CORE, \
 						"cpufreq-core", msg)
 
+#ifdef CONFIG_CPU_FREQ_OVERRIDE
+int cpufreq_override_driver_init(void);
+void cpufreq_override_driver_exit(void);
+#endif
+
 /**
  * The "cpufreq driver" - the arch- or hardware-dependent low
  * level driver of CPUFreq support, and its spinlock. This lock
@@ -1602,7 +1607,7 @@
 	int ret = 0;
 
 	cpufreq_debug_disable_ratelimit();
-	dprintk("setting new policy for CPU %u: %u - %u kHz\n", policy->cpu,
+	printk("setting new policy for CPU %u: %u - %u kHz\n", policy->cpu,
 		policy->min, policy->max);
 
 	memcpy(&policy->cpuinfo, &data->cpuinfo,
@@ -1639,7 +1644,7 @@
 	data->min = policy->min;
 	data->max = policy->max;
 
-	dprintk("new min and max freqs are %u - %u kHz\n",
+	printk("new min and max freqs are %u - %u kHz\n",
 					data->min, data->max);
 
 	if (cpufreq_driver->setpolicy) {
@@ -1651,7 +1656,7 @@
 			/* save old, working values */
 			struct cpufreq_governor *old_gov = data->governor;
 
-			dprintk("governor switch\n");
+			printk("governor switch\n");
 
 			/* end old governor */
 			if (data->governor)
@@ -1661,7 +1666,7 @@
 			data->governor = policy->governor;
 			if (__cpufreq_governor(data, CPUFREQ_GOV_START)) {
 				/* new governor failed, so re-start old one */
-				dprintk("starting governor %s failed\n",
+				printk("starting governor %s failed\n",
 							data->governor->name);
 				if (old_gov) {
 					data->governor = old_gov;
@@ -1673,7 +1678,7 @@
 			}
 			/* might be a policy change, too, so fall through */
 		}
-		dprintk("governor: change or update limits\n");
+		printk("governor: change or update limits\n");
 		__cpufreq_governor(data, CPUFREQ_GOV_LIMITS);
 	}
 
@@ -1682,6 +1687,18 @@
 	return ret;
 }
 
+#ifdef CONFIG_CPU_FREQ_OVERRIDE
+int cpufreq_set_policy(struct cpufreq_policy *policy)
+{
+	struct cpufreq_policy *data = cpufreq_cpu_get(0);
+	__cpufreq_set_policy(data,policy);
+	data->user_policy.min = data->min;
+	data->user_policy.max = data->max;
+	cpufreq_cpu_put(data);
+}
+EXPORT_SYMBOL(cpufreq_set_policy);
+#endif
+
 /**
  *	cpufreq_update_policy - re-evaluate an existing cpufreq policy
  *	@cpu: CPU which shall be re-evaluated
@@ -1701,7 +1718,7 @@
 	if (unlikely(lock_policy_rwsem_write(cpu)))
 		return -EINVAL;
 
-	dprintk("updating policy for CPU %u\n", cpu);
+	printk("updating policy for CPU %u\n", cpu);
 	memcpy(&policy, data, sizeof(struct cpufreq_policy));
 	policy.min = data->user_policy.min;
 	policy.max = data->user_policy.max;
@@ -1831,6 +1848,10 @@
 		cpufreq_debug_enable_ratelimit();
 	}
 
+#ifdef CONFIG_CPU_FREQ_OVERRIDE
+	cpufreq_override_driver_init();
+#endif
+
 	return (ret);
 }
 EXPORT_SYMBOL_GPL(cpufreq_register_driver);
@@ -1864,6 +1885,10 @@
 	cpufreq_driver = NULL;
 	spin_unlock_irqrestore(&cpufreq_driver_lock, flags);
 
+#ifdef CONFIG_CPU_FREQ_OVERRIDE
+	cpufreq_override_driver_exit();
+#endif
+
 	return 0;
 }
 EXPORT_SYMBOL_GPL(cpufreq_unregister_driver);
--- linux-2.6.24-palm/arch/arm/mach-omap3pe/prcm_opp.c	2010-06-11 17:48:34.000000000 -0400
+++ linux-2.6.24-ss/arch/arm/mach-omap3pe/prcm_opp.c	2010-11-02 23:24:57.000000000 -0400
@@ -1523,3 +1578,38 @@
 
 	return -1;
 }
+
+#ifdef CONFIG_CPU_FREQ_OVERRIDE
+void omap_pm_opp_get_volts(u8 *vdd1_volts[]) {
+	memcpy(vdd1_volts,mpu_iva2_vdd1_volts,sizeof(mpu_iva2_vdd1_volts));
+}
+EXPORT_SYMBOL(omap_pm_opp_get_volts);
+
+void omap_pm_opp_set_volts(u8 vdd1_volts[]) {
+	memcpy(mpu_iva2_vdd1_volts,vdd1_volts,sizeof(mpu_iva2_vdd1_volts));
+	prcm_do_voltage_scaling(current_vdd1_opp, current_vdd1_opp-1);
+}
+EXPORT_SYMBOL(omap_pm_opp_set_volts);
+
+void omap_pm_opp_get_vdd2_volts(u8 *vdd2_volts[]) {
+        memcpy(vdd2_volts,core_l3_vdd2_volts,sizeof(core_l3_vdd2_volts));
+}
+EXPORT_SYMBOL(omap_pm_opp_get_vdd2_volts);
+
+void omap_pm_opp_set_vdd2_volts(u8 vdd2_volts[]) {
+        memcpy(core_l3_vdd2_volts,vdd2_volts,sizeof(core_l3_vdd2_volts));
+        prcm_do_voltage_scaling(current_vdd2_opp, current_vdd2_opp-1);
+}
+EXPORT_SYMBOL(omap_pm_opp_set_vdd2_volts);
+
+void omap_pm_opp_get_vdd2_freq(u8 *vdd2_freqs[]) {
+	int i;
+	u8 f[MAX_VDD2_OPP];
+
+	for(i=0;i < MAX_VDD2_OPP;i++)
+		f[i]=(u8 )vdd2_core_freq[i].freq;
+	memcpy(vdd2_freqs,f,sizeof(f));
+}
+EXPORT_SYMBOL(omap_pm_opp_get_vdd2_freq);
+
+#endif
--- linux-2.6.24-palm/drivers/cpufreq/cpufreq_override.c	1969-12-31 19:00:00.000000000 -0500
+++ linux-2.6.24-ss/drivers/cpufreq/cpufreq_override.c	2010-11-02 23:01:54.000000000 -0400
@@ -0,0 +1,407 @@
+/*
+ *  drivers/cpufreq/cpufreq_override.c
+ *
+ *  	Marco Benton <marco@unixpsycho.com>.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include <linux/kernel.h>
+#include <linux/sysdev.h>
+#include <linux/cpu.h>
+#include <linux/sysfs.h>
+#include <linux/cpufreq.h>
+#include <linux/jiffies.h>
+#include <linux/kobject.h>
+#include <linux/workqueue.h>
+
+// VDD1 Vsel max
+#define VDD1_VSEL_MAX 112
+
+// VDD1 Vsel min
+#define VDD1_VSEL_MIN 25
+
+// VDD2 Vsel max
+#define VDD2_VSEL_MAX 55
+
+// VDD2 Vsel min
+#define VDD2_VSEL_MIN 25
+
+// High temp alarm and cap
+#define HIGHTEMP_SCALEBACK 55
+
+//Reset temp from alarm
+#define LOWTEMP_RESET 50
+
+// Polling frequency jiffies
+#define OVERRIDE_POLLING 1000
+
+// Battery scaleback percent
+#define BATTERY_PERCENT 25
+
+// Battery scaleback speed
+#define BATTERY_PERCENT_SPEED 500000
+
+void omap_pm_opp_get_volts(u8 *vdd1_volts[]);
+void omap_pm_opp_set_volts(u8 vdd1_volts[]);
+void omap_pm_opp_get_vdd2_volts(u8 *vdd2_volts[]);
+void omap_pm_opp_set_vdd2_volts(u8 vdd2_volts[]);
+void omap_pm_opp_get_vdd2_freq(u8 *vdd2_freqs[]);
+int omap34xx_get_temp(void);
+int cpufreq_set_policy(struct cpufreq_policy *policy);
+int ds2784_getpercent(int *ret_percent);
+
+static inline void check_stuff(struct work_struct *work);
+int prev_maxspeed_temp=0;
+int prev_minspeed_temp=0;
+int prev_maxspeed_bat=0;
+int prev_minspeed_bat=0;
+#ifdef CONFIG_CPU_FREQ_GOV_SCREENSTATE
+u8 charger_override=0;
+#endif
+
+static u32 override_hightemp=HIGHTEMP_SCALEBACK;
+static u32 override_lowtemp=LOWTEMP_RESET;
+static int battery_scaleback_percent=BATTERY_PERCENT;
+static int battery_scaleback_speed=BATTERY_PERCENT_SPEED;
+
+static unsigned int overtemp_alarm=0;
+static unsigned int battery_alarm=0;
+
+static DEFINE_MUTEX(override_mutex);
+static DECLARE_DELAYED_WORK(dbs_work, check_stuff);
+
+#define CPUFREQ_OVERRIDE_ATTR(_name,_mode,_show,_store) \
+static struct freq_attr _attr_##_name = {\
+        .attr = {.name = __stringify(_name), .mode = _mode, }, \
+        .show = _show,\
+        .store = _store,\
+};
+
+#define CPUFREQ_OVERRIDE_ATTR2(_name,_mode,_show) \
+static struct freq_attr _attr_##_name = {\
+	.attr = {.name = __stringify(_name), .mode = _mode, }, \
+	.show = _show,\
+};
+
+static inline void check_stuff(struct work_struct *work) {
+        struct cpufreq_policy new_policy, *policy = cpufreq_cpu_get(0);
+        u32 cputemp;
+	int battery_per;
+
+        mutex_lock(&override_mutex);
+        cputemp = omap34xx_get_temp();    // Get CPU temp
+	ds2784_getpercent(&battery_per);  // Get battery percent left
+
+	// Check values in case driver hasnt polled
+	battery_per = (battery_per > 0) ? battery_per : 100;
+	cputemp = (cputemp < 100) ? cputemp : 0;
+
+        if(cputemp > override_hightemp) {
+                if(!overtemp_alarm) {
+                        printk("CPUfreq: CPU temp warning! %dC\n",cputemp);
+                        overtemp_alarm = 1;
+                        cpufreq_get_policy(&new_policy,0);
+                        prev_minspeed_temp=policy->min;
+                        prev_maxspeed_temp=policy->max;
+                        new_policy.min=500000;
+                        new_policy.max=500000;
+			cpufreq_set_policy(&new_policy);
+                }
+        }
+        else {
+                if((overtemp_alarm) && (cputemp < override_lowtemp)) {
+                        printk("CPUfreq: CPU temp back under control! %dC\n",
+								cputemp);
+                        if (overtemp_alarm) {
+				cpufreq_get_policy(&new_policy,0);
+				new_policy.min=prev_minspeed_temp;
+				new_policy.max=prev_maxspeed_temp;
+				cpufreq_set_policy(&new_policy);
+                        	overtemp_alarm = 0;
+			}
+                }
+        }
+
+	if(battery_per<battery_scaleback_percent) {
+		if((!battery_alarm) && (!overtemp_alarm)) {
+			printk("CPUFreq: battery low! < %d%%\n",battery_per);
+			battery_alarm = 1;
+			// TODO: clean this up to not call all this code twice
+			cpufreq_get_policy(&new_policy,0);
+			prev_minspeed_bat=policy->min;
+			prev_maxspeed_bat=policy->max;
+			new_policy.min=battery_scaleback_speed;
+			new_policy.max=battery_scaleback_speed;
+			cpufreq_set_policy(&new_policy);
+		}
+	}
+	else {
+		if((battery_alarm) && (!overtemp_alarm)) {
+			printk("CPUFreq: battery OK\n");
+			cpufreq_get_policy(&new_policy,0);
+			new_policy.min=prev_minspeed_bat;
+			new_policy.max=prev_maxspeed_bat;
+			cpufreq_set_policy(&new_policy);
+			battery_alarm = 0;
+		}
+	}
+
+        mutex_unlock(&override_mutex);
+	schedule_delayed_work(&dbs_work,OVERRIDE_POLLING);
+}
+
+static ssize_t show_vdd1_vsel_max(struct cpufreq_policy *policy, char *buf) {
+	return sprintf(buf, "%hu\n",VDD1_VSEL_MAX);
+}
+
+static ssize_t show_vdd1_vsel_min(struct cpufreq_policy *policy, char *buf) {
+	return sprintf(buf, "%hu\n",VDD1_VSEL_MIN);
+}
+
+static ssize_t show_vdd1_vsel(struct cpufreq_policy *policy, char *buf) {
+        u8 volt[7];
+
+        omap_pm_opp_get_volts(&volt);
+#ifdef CONFIG_CPU_FREQ_OVERRIDE_STRIPOPP
+        return sprintf(buf, "%hu %hu %hu %hu %hu\n", volt[6],
+                                        volt[5],volt[4],volt[3],
+                                        volt[2]);
+#else
+        return sprintf(buf, "%hu %hu %hu %hu %hu %hu %hu\n", volt[6],
+                                        volt[5],volt[4],volt[3],
+                                        volt[2],volt[1],volt[0]);
+#endif
+}
+
+static ssize_t show_vdd2_vsel_max(struct cpufreq_policy *policy, char *buf) {
+        return sprintf(buf, "%hu\n",VDD2_VSEL_MAX);
+}
+
+static ssize_t show_vdd2_vsel_min(struct cpufreq_policy *policy, char *buf) {
+        return sprintf(buf, "%hu\n",VDD2_VSEL_MIN);
+}
+
+static ssize_t show_vdd2_vsel(struct cpufreq_policy *policy, char *buf) {
+        u8 volt[3];
+
+        omap_pm_opp_get_vdd2_volts(&volt);
+        return sprintf(buf, "%hu %hu %hu\n", volt[2],
+                                        volt[1],volt[0]);
+}
+
+static ssize_t show_vdd2_freqs(struct cpufreq_policy *policy, char *buf) {
+        u8 freqs[3];
+        omap_pm_opp_get_vdd2_freq(&freqs);
+
+        return sprintf(buf, "%hu %hu %hu\n", freqs[2],
+                                        freqs[1],freqs[0]);
+}
+
+static ssize_t store_vdd1_vsel(struct cpufreq_policy *policy, char *buf,
+						size_t count) {
+        u8 volt[7], i;
+
+        mutex_lock(&override_mutex);
+#ifdef CONFIG_CPU_FREQ_OVERRIDE_STRIPOPP
+        if(sscanf(buf, "%hhu %hhu %hhu %hhu %hhu", &volt[6],&volt[5],
+                                                &volt[4],&volt[3],&volt[2]) == 5) {
+		for(i=0;i < 5;i++) {
+#else
+        if(sscanf(buf, "%hhu %hhu %hhu %hhu %hhu %hhu %hhu", &volt[6],&volt[5],
+                                                &volt[4],&volt[3],&volt[2],
+                                                &volt[1],&volt[0]) == 7) {
+		for(i=0;i < 7;i++) {
+#endif
+			if((volt[i] < VDD1_VSEL_MIN) || (volt[i] >
+							VDD1_VSEL_MAX)) {
+				printk("CPUfreq: invalid vsel\n");
+				break;
+			}
+		}
+#ifdef CONFIG_CPU_FREQ_OVERRIDE_STRIPOPP
+		if(i == 5) omap_pm_opp_set_volts(volt);
+#else
+		if(i == 7) omap_pm_opp_set_volts(volt);
+#endif
+	}
+	else printk("CPUfreq: missing vsel values\n");
+
+        mutex_unlock(&override_mutex);
+	return count;
+}
+
+static ssize_t store_vdd2_vsel(struct cpufreq_policy *policy, char *buf,
+                                                size_t count) {
+        u8 volt[3], i;
+
+        mutex_lock(&override_mutex);
+        if(sscanf(buf, "%hhu %hhu %hhu", &volt[2],&volt[1], &volt[0]) == 3) {
+                for(i=0;i < 3;i++) {
+                        if((volt[i] < VDD2_VSEL_MIN) || (volt[i] >
+                                                        VDD2_VSEL_MAX)) {
+                                printk("CPUfreq: invalid vsel\n");
+                                break;
+                        }
+                }
+                if(i == 3) omap_pm_opp_set_vdd2_volts(volt);
+        }
+        else printk("CPUfreq: missing vsel values\n");
+
+        mutex_unlock(&override_mutex);
+        return count;
+}
+
+
+static ssize_t show_hightemp_scaleback(struct cpufreq_policy *policy,
+						char *buf) {
+        return sprintf(buf, "%d\n", override_hightemp);
+}
+
+static ssize_t store_hightemp_scaleback(struct cpufreq_policy *policy,
+						char *buf, size_t count) {
+        int maxtemp=0;
+
+        if(sscanf(buf, "%d", &maxtemp) == 1)
+                override_hightemp=(maxtemp) ? maxtemp : HIGHTEMP_SCALEBACK;
+        else printk("CPUfreq: invalid max temp\n");
+
+	return count;
+}
+
+static ssize_t show_battery_scaleback_per(struct cpufreq_policy *policy,
+                                                char *buf) {
+	return sprintf(buf, "%d\n", battery_scaleback_percent);
+}
+
+static ssize_t store_battery_scaleback_per(struct cpufreq_policy *policy,
+                                                char *buf, size_t count) {
+	int bat=0;
+
+	if(sscanf(buf, "%d", &bat) == 1)
+		battery_scaleback_percent=((bat > -1) && (bat<100))
+						? bat : BATTERY_PERCENT;
+	else printk("CPUfreq: invalid battery percentage\n");
+
+	return count;
+}
+
+static ssize_t show_battery_scaleback_speed(struct cpufreq_policy *policy,
+                                                char *buf) {
+	return sprintf(buf, "%d\n", battery_scaleback_speed);
+}
+
+static ssize_t store_battery_scaleback_speed(struct cpufreq_policy *policy,
+                                                char *buf, size_t count) {
+	int bat=0;
+
+	if(sscanf(buf, "%d", &bat) == 1)
+		battery_scaleback_speed=(bat>125000)
+						? bat : BATTERY_PERCENT_SPEED;
+	else printk("CPUfreq: invalid battery scaleback speed\n");
+
+	return count;
+}
+
+static ssize_t show_lowtemp_reset(struct cpufreq_policy *policy, char *buf) {
+
+        return sprintf(buf, "%d\n", override_lowtemp);
+}
+
+static ssize_t store_lowtemp_reset(struct cpufreq_policy *policy, char *buf,
+						size_t count) {
+        int lowtemp=0;
+
+        if(sscanf(buf, "%d", &lowtemp) == 1)
+                override_lowtemp=(lowtemp) ? lowtemp : LOWTEMP_RESET;
+        else printk("CPUfreq: invalid low temp\n");
+
+	return count;
+}
+
+#ifdef CONFIG_CPU_FREQ_GOV_SCREENSTATE
+static ssize_t show_charger_override(struct cpufreq_policy *policy, char *buf) {
+	return sprintf(buf, "%hu\n",charger_override);
+}
+
+static ssize_t store_charger_override(struct cpufreq_policy *policy,
+					char *buf, size_t count) {
+	u8 override=0;
+
+	if(sscanf(buf, "%hhu", &override) == 1)
+		charger_override=(override) ? 1 : 0;
+	else printk("CPUfreq: invalid charger override value\n");
+
+	return count;
+}
+
+int override_show_chrg_ovrd() {
+	return charger_override;
+}
+EXPORT_SYMBOL(override_show_chrg_ovrd);
+
+CPUFREQ_OVERRIDE_ATTR(override_charger,0644,show_charger_override,
+			store_charger_override);
+#endif
+
+CPUFREQ_OVERRIDE_ATTR(vdd1_vsel,0644,show_vdd1_vsel,store_vdd1_vsel);
+CPUFREQ_OVERRIDE_ATTR(vdd2_vsel,0644,show_vdd2_vsel,store_vdd2_vsel);
+CPUFREQ_OVERRIDE_ATTR(battery_scaleback_percent,0644,
+			show_battery_scaleback_per,
+			store_battery_scaleback_per);
+CPUFREQ_OVERRIDE_ATTR(battery_scaleback_speed,0644,
+			show_battery_scaleback_speed,
+			store_battery_scaleback_speed);
+CPUFREQ_OVERRIDE_ATTR2(vdd1_vsel_min,0444,show_vdd1_vsel_min);
+CPUFREQ_OVERRIDE_ATTR2(vdd1_vsel_max,0444,show_vdd1_vsel_max);
+CPUFREQ_OVERRIDE_ATTR2(vdd2_vsel_min,0444,show_vdd2_vsel_min);
+CPUFREQ_OVERRIDE_ATTR2(vdd2_vsel_max,0444,show_vdd2_vsel_max);
+CPUFREQ_OVERRIDE_ATTR2(vdd2_freqs,0444,show_vdd2_freqs);
+CPUFREQ_OVERRIDE_ATTR(cpu_hightemp_alarm,0644,show_hightemp_scaleback,
+			store_hightemp_scaleback);
+CPUFREQ_OVERRIDE_ATTR(cpu_hightemp_reset,0644,show_lowtemp_reset,
+			store_lowtemp_reset);
+
+static struct attribute *default_attrs[] = {
+        &_attr_vdd1_vsel.attr,
+        &_attr_vdd1_vsel_min.attr,
+        &_attr_vdd1_vsel_max.attr,
+        &_attr_vdd2_vsel.attr,
+        &_attr_vdd2_vsel_min.attr,
+        &_attr_vdd2_vsel_max.attr,
+        &_attr_vdd2_freqs.attr,
+        &_attr_cpu_hightemp_alarm.attr,
+        &_attr_cpu_hightemp_reset.attr,
+	&_attr_battery_scaleback_percent.attr,
+	&_attr_battery_scaleback_speed.attr,
+#ifdef CONFIG_CPU_FREQ_GOV_SCREENSTATE
+	&_attr_override_charger.attr,
+#endif
+        NULL
+};
+
+static struct attribute_group override_attr_group = {
+        .attrs = default_attrs,
+        .name = "override"
+};
+
+int cpufreq_override_driver_init(void) {
+	schedule_delayed_work(&dbs_work,OVERRIDE_POLLING);
+        struct cpufreq_policy *data = cpufreq_cpu_get(0);
+        return sysfs_create_group(&data->kobj,&override_attr_group);
+}
+EXPORT_SYMBOL(cpufreq_override_driver_init);
+
+void cpufreq_override_driver_exit(void) {
+	struct cpufreq_policy *policy = cpufreq_cpu_get(0);
+	cancel_delayed_work(&dbs_work);
+	sysfs_remove_group(&policy->kobj, &override_attr_group);
+	flush_scheduled_work();
+}
+EXPORT_SYMBOL(cpufreq_override_driver_exit);
+
+MODULE_AUTHOR ("marco@unixpsycho.com");
+MODULE_DESCRIPTION ("'cpufreq_override' - A driver to do cool stuff ");
+MODULE_LICENSE ("GPL");
--- linux-2.6.24-palm/drivers/hwmon/omap34xx_temp.c     2010-05-31 12:05:29.000000000 -0400
+++ linux-2.6.24-ss/drivers/hwmon/omap34xx_temp.c   2010-07-11 20:46:13.000000000 -0400
@@ -140,6 +140,14 @@
	mutex_unlock(&data->update_lock);
 }

+int omap34xx_get_temp(void) {
+	struct omap34xx_data *data =
+		dev_get_drvdata(&omap34xx_temp_device.dev);
+	omap34xx_update(data);
+	return adc_to_temp[omap_ctrl_readl(OMAP343X_CONTROL_TEMP_SENSOR) & ((1<<7) - 1)];
+}
+EXPORT_SYMBOL(omap34xx_get_temp);
+
 static ssize_t show_name(struct device *dev,
 		struct device_attribute *devattr, char *buf)
 {
--- linux-2.6.24-palm/arch/arm/mach-omap3pe/clock.c	2010-06-11 17:48:34.000000000 -0400
+++ linux-2.6.24-ss/arch/arm/mach-omap3pe/clock.c	2010-11-17 15:36:51.000000000 -0500
@@ -709,6 +709,10 @@
 
 	prcm = vdd1_rate_table + ARRAY_SIZE(vdd1_rate_table) -1;
 	for (; prcm->speed; prcm--) {
+#ifdef CONFIG_CPU_FREQ_OVERRIDE_STRIPOPP
+		// Damn l-users!!!!  This will fix them!!!
+		if((prcm->speed / 1000) < 500000) continue;
+#endif
 		freq_table[i].index = i;
 		freq_table[i].frequency = prcm->speed / 1000;
 		i++;
--- linux-2.6.24-palm/drivers/w1/slaves/w1_ds2784.c	2010-06-11 17:48:35.000000000 -0400
+++ linux-2.6.24-ss/drivers/w1/slaves/w1_ds2784.c	2010-08-31 00:26:02.000000000 -0400
@@ -1133,11 +1133,20 @@
 */
 static struct device *battery_device = NULL;
 
+#ifdef CONFIG_CPU_FREQ_OVERRIDE
+int ds2784_getpercent(int *ret_percent)
+{
+        if (!battery_device) return -1;
+        return ds2784_getpercent_dev(battery_device, ret_percent);
+}
+EXPORT_SYMBOL(ds2784_getpercent);
+#else
 static int ds2784_getpercent(int *ret_percent)
 {
 	if (!battery_device) return -1;
 	return ds2784_getpercent_dev(battery_device, ret_percent);
 }
+#endif
 
 static int ds2784_getvoltage(int *ret_voltage)
 {
@@ -1151,11 +1160,20 @@
 	return ds2784_gettemperature_dev(battery_device, ret_temperature);
 }
 
+#ifdef CONFIG_CPU_FREQ_GOV_SCREENSTATE
+int ds2784_getcurrent(int *ret_current)
+{
+        if (!battery_device) return -1;
+        return ds2784_getcurrent_dev(battery_device, ret_current);
+}
+EXPORT_SYMBOL(ds2784_getcurrent);
+#else
 static int ds2784_getcurrent(int *ret_current)
 {
 	if (!battery_device) return -1;
 	return ds2784_getcurrent_dev(battery_device, ret_current);
 }
+#endif
 
 static struct battery_ops ds2784_battery_ops = {
 	.get_percent       = ds2784_getpercent,
--- linux-2.6.24-palm/include/linux/cpufreq.h	2010-06-11 17:48:35.000000000 -0400
+++ linux-2.6.24-ss/include/linux/cpufreq.h	2010-12-23 09:47:36.000000000 -0500
@@ -288,6 +288,52 @@
 }
 #endif
 
+/*********************************************************************
+ *                       CPUFREQ ONDEMAND TICKLE                     *
+ *********************************************************************/
+#ifdef CONFIG_CPU_FREQ_GOV_ONDEMAND_TICKLE
+void cpufreq_ondemand_tickle(void);
+void cpufreq_ondemand_tickle_millis(unsigned int millis);
+void cpufreq_ondemand_floor(unsigned int freq);
+void cpufreq_ondemand_floor_millis(unsigned int freq, unsigned int millis);
+void cpufreq_ondemand_hold(void);
+void cpufreq_ondemand_unhold(void);
+void cpufreq_ondemand_hold_check(int *flag);
+void cpufreq_ondemand_unhold_check(int *flag);
+void cpufreq_ondemand_floor_hold(unsigned int freq);
+void cpufreq_ondemand_floor_unhold(void); 
+void cpufreq_ondemand_floor_hold_check(unsigned int freq, int *flag);
+void cpufreq_ondemand_floor_unhold_check(int *flag);
+#define CPUFREQ_TICKLE cpufreq_ondemand_tickle
+#define CPUFREQ_TICKLE_MILLIS(millis) cpufreq_ondemand_tickle_millis((millis))
+#define CPUFREQ_FLOOR cpufreq_ondemand_floor
+#define CPUFREQ_FLOOR_MILLIS(freq, millis) \
+        cpufreq_ondemand_floor_millis((freq), (millis))
+#define CPUFREQ_HOLD cpufreq_ondemand_hold
+#define CPUFREQ_UNHOLD cpufreq_ondemand_unhold
+#define CPUFREQ_HOLD_CHECK cpufreq_ondemand_hold_check
+#define CPUFREQ_UNHOLD_CHECK cpufreq_ondemand_unhold_check
+#define CPUFREQ_FLOOR_HOLD cpufreq_ondemand_floor_hold
+#define CPUFREQ_FLOOR_UNHOLD cpufreq_ondemand_floor_unhold
+#define CPUFREQ_FLOOR_HOLD_CHECK cpufreq_ondemand_floor_hold_check
+#define CPUFREQ_FLOOR_UNHOLD_CHECK cpufreq_ondemand_floor_unhold_check
+
+#else
+
+#define CPUFREQ_TICKLE()
+#define CPUFREQ_TICKLE_MILLIS(millis)
+#define CPUFREQ_FLOOR()
+#define CPUFREQ_FLOOR_MILLIS(freq, millis)
+#define CPUFREQ_HOLD()
+#define CPUFREQ_UNHOLD()
+#define CPUFREQ_HOLD_CHECK(flag)
+#define CPUFREQ_UNHOLD_CHECK(flag)
+#define CPUFREQ_FLOOR_HOLD(freq)
+#define CPUFREQ_FLOOR_UNHOLD()
+#define CPUFREQ_FLOOR_HOLD_CHECK(freq, flag)
+#define CPUFREQ_FLOOR_UNHOLD_CHECK(flag)
+
+#endif
 
 /*********************************************************************
  *                       CPUFREQ DEFAULT GOVERNOR                    *
@@ -309,9 +355,18 @@
 #elif defined(CONFIG_CPU_FREQ_DEFAULT_GOV_USERSPACE)
 extern struct cpufreq_governor cpufreq_gov_userspace;
 #define CPUFREQ_DEFAULT_GOVERNOR	(&cpufreq_gov_userspace)
+#elif defined(CONFIG_CPU_FREQ_DEFAULT_GOV_SCREENSTATE)
+extern struct cpufreq_governor cpufreq_gov_screenstate;
+#define CPUFREQ_DEFAULT_GOVERNOR	(&cpufreq_gov_screenstate)
+/* #elif defined(CONFIG_CPU_FREQ_DEFAULT_GOV_VDEMAND)
+ extern struct cpufreq_governor cpufreq_gov_vdemand;
+#define CPUFREQ_DEFAULT_GOVERNOR	(&cpufreq_gov_vdemand) */
 #elif defined(CONFIG_CPU_FREQ_DEFAULT_GOV_ONDEMAND)
 extern struct cpufreq_governor cpufreq_gov_ondemand;
 #define CPUFREQ_DEFAULT_GOVERNOR	(&cpufreq_gov_ondemand)
+#elif defined(CONFIG_CPU_FREQ_DEFAULT_GOV_ONDEMAND_TICKLE)
+extern struct cpufreq_governor cpufreq_gov_ondemand_tickle;
+#define CPUFREQ_DEFAULT_GOVERNOR        (&cpufreq_gov_ondemand_tickle)
 #elif defined(CONFIG_CPU_FREQ_DEFAULT_GOV_CONSERVATIVE)
 extern struct cpufreq_governor cpufreq_gov_conservative;
 #define CPUFREQ_DEFAULT_GOVERNOR	(&cpufreq_gov_conservative)
--- linux-2.6.24-palm/drivers/video/omap/lcd_panel.c	2010-06-11 17:48:35.000000000 -0400
+++ linux-2.6.24-ss/drivers/video/omap/lcd_panel.c	2010-08-25 14:48:12.000000000 -0400
@@ -31,6 +31,11 @@
 
 #include "lcd.h"
 
+#ifdef CONFIG_CPU_FREQ_GOV_SCREENSTATE
+void cpufreq_gov_screenstate_lcdon(void);
+void cpufreq_gov_screenstate_lcdoff(void);
+#endif
+
 #define MOD_NAME 		"LCD: "
 
 #undef MODDEBUG
@@ -52,7 +57,6 @@
 #define DISPLAY_BACKLIGHT_STATE_ON     1
 #define DISPLAY_BACKLIGHT_STATE_OFF    0
 
-
 struct lcd_params {
 	struct display_device *disp_dev;
 	struct platform_device *pdev;
@@ -128,6 +132,11 @@
 						DISPLAY_BACKLIGHT_STATE_ON);
 		}
 		params->panel_state = DISPLAY_DEVICE_STATE_ON;
+
+#ifdef CONFIG_CPU_FREQ_GOV_SCREENSTATE
+		cpufreq_gov_screenstate_lcdon();
+#endif
+
 	} else {
 		if (params->panel_state == DISPLAY_DEVICE_STATE_OFF) {
 			DPRINTK(" %s:  Panel already off, returning...\n",
@@ -156,6 +165,10 @@
 						DISPLAY_CONTROLLER_STATE_OFF);
 		}
 		params->panel_state = DISPLAY_DEVICE_STATE_OFF;
+
+#ifdef CONFIG_CPU_FREQ_GOV_SCREENSTATE
+		cpufreq_gov_screenstate_lcdoff();
+#endif
 	}
 
 unlock:
--- linux-2.6.24-palm/drivers/usb/gadget/gadget_event.c	2010-06-11 17:48:34.000000000 -0400
+++ linux-2.6.24-ss/drivers/usb/gadget/gadget_event.c	2010-08-25 14:48:12.000000000 -0400
@@ -282,6 +282,13 @@
 }
 EXPORT_SYMBOL(gadget_event_power_state_changed);
 
+#ifdef CONFIG_CPU_FREQ_GOV_SCREENSTATE
+int gadget_event_state_current(void) {
+	return the_state.current_mA;
+}
+EXPORT_SYMBOL(gadget_event_state_current);
+#endif
+
 static int __init init(void)
 {
 	int ret = 0;
--- linux-2.6.24-palm/drivers/cpufreq/cpufreq_screenstate.c	1969-12-31 19:00:00.000000000 -0500
+++ linux-2.6.24-ss/drivers/cpufreq/cpufreq_screenstate.c	2010-08-28 09:30:14.000000000 -0400
@@ -0,0 +1,212 @@
+/*
+ *  linux/drivers/cpufreq/cpufreq_screenstate.c
+ *
+ *  Marco Benton marco@unixpsycho.com 
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/spinlock.h>
+#include <linux/interrupt.h>
+#include <linux/cpufreq.h>
+#include <linux/cpu.h>
+#include <linux/types.h>
+#include <linux/mutex.h>
+#include <asm/uaccess.h>
+#include <linux/workqueue.h>
+
+// Cap min freq capped to 500MHz, undef to set to policy->min
+//#define SCREENSTATE_CAP_MIN_FREQ
+
+static unsigned int cpu_is_managed=0;
+static unsigned int lcd_state;
+static unsigned int charging_state;
+
+int gadget_event_state_current(void);
+static int ds2784_getcurrent(int *ret_current);
+static inline void check_charger(struct work_struct *work);
+
+static DEFINE_MUTEX(screenstate_mutex);
+
+static DECLARE_DELAYED_WORK(dbs_work, check_charger);
+
+#ifdef CONFIG_CPU_FREQ_OVERRIDE
+int override_show_chrg_ovrd();
+u8 ch_override;
+#endif
+
+static inline void check_charger(struct work_struct *work) {
+	struct cpufreq_policy *policy = cpufreq_cpu_get(0);
+	int cur=0,current_mA=0;
+
+	mutex_lock(&screenstate_mutex);
+#ifdef CONFIG_CPU_FREQ_OVERRIDE
+	int oc = override_show_chrg_ovrd();
+
+	if((oc) && (!ch_override)) {
+		printk("screenstate: charger override set\n");
+		charging_state=0;
+		ch_override=1;
+		if(lcd_state) __cpufreq_driver_target(policy, policy->max,
+							CPUFREQ_RELATION_H);
+	}
+	else if((!oc) && (ch_override)) {
+		ch_override=0;
+		printk("screenstate: charger override off\n");
+	}
+
+	if(ch_override) goto out;
+#endif
+
+	ds2784_getcurrent(&cur);
+	current_mA=gadget_event_state_current();
+	if((cur>0) && (current_mA < 500)) {
+		// Assume Touchstone
+		if(!charging_state) {
+			charging_state=1;
+			__cpufreq_driver_target(policy, 500000,
+						CPUFREQ_RELATION_L);
+			printk("screenstate: TS found!\n");
+		}
+	} 
+	else {
+		if(current_mA == 1000) {
+			if(!charging_state) {
+				charging_state=1;
+				__cpufreq_driver_target(policy, 500000,
+							CPUFREQ_RELATION_L);
+				printk("screenstate: 1000mA charger found!\n");
+			}
+		}
+		else {
+			if(charging_state) {
+				charging_state=0;
+				printk("screenstate: charger unplugged!\n");
+				if(lcd_state)
+					__cpufreq_driver_target(policy,
+						policy->max, CPUFREQ_RELATION_H);
+			}
+		}
+	}
+#ifdef CONFIG_CPU_FREQ_OVERRIDE
+out:
+#endif
+	schedule_delayed_work(&dbs_work,1000);
+	mutex_unlock(&screenstate_mutex);
+	return;
+}
+
+static int cpufreq_governor_screenstate(struct cpufreq_policy *policy,
+				   unsigned int event) {
+
+	switch (event) {
+		case CPUFREQ_GOV_START:
+			if(cpu_is_managed) break;
+
+			cpu_is_managed = 1;
+			lcd_state = 1;
+			charging_state = 0;
+#ifdef CONFIG_CPU_FREQ_OVERRIDE
+			ch_override = override_show_chrg_ovrd();
+#endif
+			mutex_lock(&screenstate_mutex);
+			__cpufreq_driver_target(policy, policy->max,
+							CPUFREQ_RELATION_H);
+			schedule_delayed_work(&dbs_work,1000);
+			mutex_unlock(&screenstate_mutex);
+			
+			break;
+		case CPUFREQ_GOV_STOP:
+			cpu_is_managed = 0;
+			lcd_state = 0;
+#ifdef CONFIG_CPU_FREQ_OVERRIDE
+			ch_override = 0;
+#endif
+			cancel_delayed_work(&dbs_work);
+			break;
+		case CPUFREQ_GOV_LIMITS:
+			mutex_lock(&screenstate_mutex);
+			printk("screenstate: policy change\n");
+			if(charging_state)
+				__cpufreq_driver_target(policy, 500000,
+							CPUFREQ_RELATION_L);
+			else {
+				if(lcd_state) __cpufreq_driver_target(policy,
+					policy->max, CPUFREQ_RELATION_H);
+#ifdef SCREENSTATE_CAP_MIN_FREQ
+				else __cpufreq_driver_target(policy, 500000,
+							CPUFREQ_RELATION_L);
+#else
+				else __cpufreq_driver_target(policy,
+					policy->min, CPUFREQ_RELATION_L);
+#endif
+			}
+			mutex_unlock(&screenstate_mutex);
+			break;
+		}
+		return 0;
+}
+
+struct cpufreq_governor cpufreq_gov_screenstate = {
+	.name		= "screenstate",
+	.governor	= cpufreq_governor_screenstate,
+	.owner		= THIS_MODULE,
+};
+
+static int __init cpufreq_gov_screenstate_init(void) {
+	return cpufreq_register_governor(&cpufreq_gov_screenstate);
+}
+
+static void __exit cpufreq_gov_screenstate_exit(void) {
+	flush_scheduled_work();
+	cpufreq_unregister_governor(&cpufreq_gov_screenstate);
+}
+
+void cpufreq_gov_screenstate_lcdoff(void) {
+	struct cpufreq_policy *policy = cpufreq_cpu_get(0);
+	mutex_lock(&screenstate_mutex);
+	if(cpu_is_managed) {
+		printk("screenstate: lcd off\n");
+#ifdef SCREENSTATE_CAP_MIN_FREQ
+		__cpufreq_driver_target(policy, 500000, CPUFREQ_RELATION_L);
+#else
+		__cpufreq_driver_target(policy, policy->min, CPUFREQ_RELATION_L);
+#endif
+		lcd_state = 0;
+	}
+	mutex_unlock(&screenstate_mutex);
+}
+EXPORT_SYMBOL(cpufreq_gov_screenstate_lcdoff);
+
+void cpufreq_gov_screenstate_lcdon(void) {
+	struct cpufreq_policy *policy = cpufreq_cpu_get(0);
+	mutex_lock(&screenstate_mutex);
+        if(cpu_is_managed) {
+		printk("screenstate: lcd on\n");
+                if(!charging_state) __cpufreq_driver_target(policy,
+					policy->max, CPUFREQ_RELATION_H);
+		else __cpufreq_driver_target(policy, 500000, CPUFREQ_RELATION_L);
+		lcd_state = 1;
+        }
+	mutex_unlock(&screenstate_mutex);
+}
+EXPORT_SYMBOL(cpufreq_gov_screenstate_lcdon);
+
+EXPORT_SYMBOL(cpufreq_gov_screenstate);
+
+MODULE_AUTHOR ("marco@unixpsycho.com");
+MODULE_DESCRIPTION ("CPUfreq policy governor 'screenstate'");
+MODULE_LICENSE ("GPL");
+
+#ifdef CONFIG_CPU_FREQ_DEFAULT_GOV_SCREENSTATE
+fs_initcall(cpufreq_gov_screenstate_init);
+#else
+module_init(cpufreq_gov_screenstate_init);
+#endif
+module_exit(cpufreq_gov_screenstate_exit);
--- linux-2.6.24-palm/drivers/cpufreq/cpufreq_ondemand_tickle.c	1969-12-31 19:00:00.000000000 -0500
+++ linux-2.6.24-ss/drivers/cpufreq/cpufreq_ondemand_tickle.c	2010-12-23 23:35:52.000000000 -0500
@@ -0,0 +1,1702 @@
+/*
+ *  drivers/cpufreq/cpufreq_ondemand_tickle.c
+ *
+ *  A version of cpufreq_ondemand supporing hinting, or tickling, into
+ *  high performance levels based on platform defined events. This governor
+ *  also supports setting a temporary frequency floor for maintaining
+ *  minimum required performance levels while still conserving power, such
+ *  as may be required in codecs, com stacks, etc. Ondemand_tickle should
+ *  behave identically to ondemand when neither a tickel nor a floor is active.
+ *
+ *  Copyright (C)  2001 Russell King
+ *            (C)  2003 Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>.
+ *                      Jun Nakajima <jun.nakajima@intel.com>
+ *            (C)  2009 Palm Inc, Corey Tabaka <corey.tabaka@palm.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/cpufreq.h>
+#include <linux/cpufreq_tickle.h>
+#include <linux/cpu.h>
+#include <linux/jiffies.h>
+#include <linux/kernel_stat.h>
+#include <linux/mutex.h>
+#include <linux/proc_fs.h>
+#include <linux/seq_file.h>
+#include <linux/vmalloc.h>
+#include <linux/ioctl.h>
+#include <linux/fs.h>
+#include <linux/cdev.h>
+#include <linux/list.h>
+
+/*
+ * dbs is used in this file as a shortform for demandbased switching
+ * It helps to keep variable names smaller, simpler
+ */
+
+#define DEF_FREQUENCY_UP_THRESHOLD		(40)
+#define MIN_FREQUENCY_UP_THRESHOLD		(11)
+#define MAX_FREQUENCY_UP_THRESHOLD		(100)
+
+/*
+ * The max and default time in mS to keep the processor at max freq from a tickle.
+ */
+#define MAX_TICKLE_WINDOW				(10000)
+#define DEF_TICKLE_WINDOW				(3000)
+
+/*
+ * The max and default time in mS to keep the processor at at least the floor freq.
+ */
+#define MAX_FLOOR_WINDOW				(10000)
+#define DEF_FLOOR_WINDOW				(3000)
+
+/*
+ * The polling frequency of this governor depends on the capability of
+ * the processor. Default polling frequency is 1000 times the transition
+ * latency of the processor. The governor will work on any processor with
+ * transition latency <= 10mS, using appropriate sampling
+ * rate.
+ * For CPUs with transition latency > 10mS (mostly drivers with CPUFREQ_ETERNAL)
+ * this governor will not work.
+ * All times here are in uS.
+ */
+static unsigned int def_sampling_rate;
+#define MIN_SAMPLING_RATE_RATIO			(2)
+/* for correct statistics, we need at least 10 ticks between each measure */
+#define MIN_STAT_SAMPLING_RATE 			\
+			(MIN_SAMPLING_RATE_RATIO * jiffies_to_usecs(10))
+#define MIN_SAMPLING_RATE			\
+			(def_sampling_rate / MIN_SAMPLING_RATE_RATIO)
+#define MAX_SAMPLING_RATE			(500 * def_sampling_rate)
+#define DEF_SAMPLING_RATE_LATENCY_MULTIPLIER	(1000)
+#define TRANSITION_LATENCY_LIMIT		(10 * 1000 * 1000)
+
+static void do_dbs_timer(struct work_struct *work);
+
+static void do_tickle_timer(unsigned long arg);
+static void do_floor_timer(unsigned long arg);
+
+/* Sampling types */
+enum {DBS_NORMAL_SAMPLE, DBS_SUB_SAMPLE};
+
+struct cpu_dbs_info_s {
+	cputime64_t prev_cpu_idle;
+	cputime64_t prev_cpu_wall;
+	struct cpufreq_policy *cur_policy;
+	struct delayed_work work;
+	struct cpufreq_frequency_table *freq_table;
+	unsigned int freq_lo;
+	unsigned int freq_lo_jiffies;
+	unsigned int freq_hi_jiffies;
+
+	int tickle_active;
+	int floor_active;
+	unsigned int freq_floor;
+	unsigned int freq_save;
+	unsigned int rel_save;
+	int cur_load;
+
+	int cpu;
+	unsigned int enable:1,
+	             sample_type:1;
+};
+static DEFINE_PER_CPU(struct cpu_dbs_info_s, cpu_dbs_info);
+
+static unsigned int dbs_enable;	/* number of CPUs using this policy */
+
+static void adjust_for_load(struct cpu_dbs_info_s *this_dbs_info);
+
+/*
+ * DEADLOCK ALERT! There is a ordering requirement between cpu_hotplug
+ * lock and dbs_mutex. cpu_hotplug lock should always be held before
+ * dbs_mutex. If any function that can potentially take cpu_hotplug lock
+ * (like __cpufreq_driver_target()) is being called with dbs_mutex taken, then
+ * cpu_hotplug lock should be taken before that. Note that cpu_hotplug lock
+ * is recursive for the same process. -Venki
+ */
+static DEFINE_MUTEX(dbs_mutex);
+
+static struct workqueue_struct	*kondemand_wq;
+
+static struct dbs_tuners {
+	unsigned int sampling_rate;
+	unsigned int up_threshold;
+	unsigned int ignore_nice;
+	unsigned int powersave_bias;
+	unsigned int max_tickle_window;
+	unsigned int max_floor_window;
+} dbs_tuners_ins = {
+	.up_threshold = DEF_FREQUENCY_UP_THRESHOLD,
+	.ignore_nice = 0,
+	.powersave_bias = 1,
+	.max_tickle_window = DEF_TICKLE_WINDOW,
+	.max_floor_window = DEF_FLOOR_WINDOW,
+};
+
+/*
+ * tickle/floor state
+ */
+static struct {
+	spinlock_t lock;
+
+	int active;
+	int active_tickle;
+	int active_count;
+	unsigned long tickle_jiffies;
+
+
+	int floor_active;
+	int floor_tickle;
+	int floor_count;
+	unsigned long floor_jiffies;
+	unsigned int cur_freq_floor;
+
+	struct work_struct tickle_work;
+	struct work_struct floor_work;
+
+	struct timer_list tickle_timer;
+	struct timer_list floor_timer;
+} tickle_state = {
+	.lock = SPIN_LOCK_UNLOCKED,
+	.active = 0,
+	.active_tickle = 0,
+	.active_count = 0,
+	.floor_active = 0,
+	.floor_tickle = 0,
+	.floor_count = 0,
+	.cur_freq_floor = 0,
+};
+
+/*
+ * Stats for profiling response characteristics.
+ */
+#define NUM_SAMPLES 10000
+struct sample_data {
+	cputime64_t timestamp;
+	cputime64_t user;
+	cputime64_t system;
+	cputime64_t irq;
+	cputime64_t softirq;
+	cputime64_t steal;
+	cputime64_t nice;
+	unsigned int cur_freq;
+	unsigned int target_freq;
+	int load;
+};
+
+struct sample_stats {
+	struct sample_data *samples;
+	unsigned int current_sample;
+};
+
+static DEFINE_PER_CPU(struct sample_stats, sample_stats);
+
+static int sampling_enabled = 0;
+module_param(sampling_enabled, bool, S_IRUGO | S_IWUSR);
+
+static int tickling_enabled = 0;
+module_param(tickling_enabled, bool, S_IRUGO | S_IWUSR);
+
+static int clear_samples = 0;
+module_param(clear_samples, bool, S_IRUGO | S_IWUSR);
+
+static int print_tickles = 0;
+module_param(print_tickles, bool, S_IRUGO | S_IWUSR);
+
+/********************* procfs ********************/
+struct stats_state {
+	int cpu;
+	int sample_index;
+	int num_samples;
+};
+
+static struct seq_operations stats_op;
+
+static void *stats_start(struct seq_file *, loff_t *);
+static void *stats_next(struct seq_file *, void *v, loff_t *);
+static void stats_stop(struct seq_file *, void *);
+static int stats_show(struct seq_file *, void *);
+
+static int stats_open(struct inode *inode, struct file *file)
+{
+	return seq_open(file, &stats_op);
+}
+
+static struct file_operations proc_stats_operations = {
+	.open		= stats_open,
+	.read		= seq_read,
+	.llseek		= seq_lseek,
+	.release	= seq_release,
+};
+
+static struct seq_operations stats_op = {
+	.start		= stats_start,
+	.next		= stats_next,
+	.stop		= stats_stop,
+	.show		= stats_show,
+};
+
+static int get_num_sample_records(void) {
+	int i, count = 0;
+
+	for (i = 0; i < NR_CPUS; i++) {
+		struct sample_stats *stats = &per_cpu(sample_stats, i);
+
+		if (stats->samples)
+			count += NUM_SAMPLES;
+	}
+
+	return count;
+}
+
+static void *stats_start(struct seq_file *m, loff_t *pos)
+{
+	struct stats_state *state = kmalloc(sizeof(struct stats_state),  GFP_KERNEL);
+
+	printk(KERN_DEBUG "%s: %u\n", __FUNCTION__, (unsigned int) *pos);
+
+	if (!state)
+		return ERR_PTR(-ENOMEM);
+
+	state->cpu = 0;
+	state->sample_index = *pos;
+	state->num_samples = get_num_sample_records();
+
+	if (*pos >= state->num_samples)
+		return NULL;
+
+	while (state->sample_index > NUM_SAMPLES) {
+		struct sample_stats *stats = &per_cpu(sample_stats, state->cpu);
+
+		state->cpu++;
+		if (stats->samples)
+			state->sample_index -= NUM_SAMPLES;
+	}
+
+	return (void *) state;
+}
+
+static void *stats_next(struct seq_file *m, void *v, loff_t *pos)
+{
+	struct stats_state *state = (struct stats_state *) v;
+
+	printk(KERN_DEBUG "%s: %u\n" ,__FUNCTION__, (unsigned int) *pos);
+
+	++*pos;
+
+	if (*pos >= state->num_samples)
+		return NULL;
+
+	state->cpu = 0;
+	state->sample_index = *pos;
+
+	while (state->sample_index > NUM_SAMPLES) {
+		struct sample_stats *stats = &per_cpu(sample_stats, state->cpu);
+
+		state->cpu++;
+		if (stats->samples)
+			state->sample_index -= NUM_SAMPLES;
+	}
+
+	return (void *) state;
+}
+
+static void stats_stop(struct seq_file *m, void *v)
+{
+	kfree(v);
+}
+
+static int stats_show(struct seq_file *m, void *v)
+{
+	struct stats_state *state = v;
+	struct sample_stats *stats = &per_cpu(sample_stats, state->cpu);
+	int rc = 0;
+
+	if (state->sample_index == 0) {
+		rc = seq_printf(m, "cpufreq samples for cpu %u\n", state->cpu);
+		if (rc)
+			goto done;
+	}
+
+	if (stats->samples) {
+		rc = seq_printf(m, "%u %u: %llu %llu %llu %llu %llu %llu %llu %d %u %u\n",
+				state->cpu,
+				state->sample_index,
+				stats->samples[state->sample_index].timestamp,
+				stats->samples[state->sample_index].user,
+				stats->samples[state->sample_index].system,
+				stats->samples[state->sample_index].irq,
+				stats->samples[state->sample_index].softirq,
+				stats->samples[state->sample_index].steal,
+				stats->samples[state->sample_index].nice,
+				stats->samples[state->sample_index].load,
+				stats->samples[state->sample_index].cur_freq,
+				stats->samples[state->sample_index].target_freq
+		);
+		if (rc)
+			goto done;
+	}
+
+done:
+	return rc;
+}
+/******************** end procfs ********************/
+
+/********************** ioctl ***********************/
+
+static struct class *tickle_class = NULL;
+static struct cdev tickle_cdev;
+static dev_t tickle_dev;
+
+static LIST_HEAD(tickle_clients);
+
+struct tickle_file_data {
+	struct list_head list;
+
+	int tickle_hold_flag;
+	int floor_hold_flag;
+	unsigned int floor_freq;
+};
+
+/* static tickle_file_data entry for use by anonymous floor tickles that don't have a file handle */
+static struct tickle_file_data default_file_data;
+
+static int get_max_client_floor(void);
+
+static int tickle_open(struct inode *inode, struct file *filp);
+static int tickle_release(struct inode *inode, struct file *filp);
+static int tickle_ioctl(struct inode *inode, struct file *filp, unsigned int cmd, unsigned long arg);
+
+static struct file_operations tickle_fops = {
+	.owner			= THIS_MODULE,
+	.open			= tickle_open,
+	.release		= tickle_release,
+	.ioctl			= tickle_ioctl,
+};
+
+static int setup_tickle_device(void)
+{
+	return 0;
+// Don't need this for webOS 1.4.5... BAD!!!
+#if 0
+	int res = 0;
+	struct device *dev = NULL;
+	res = alloc_chrdev_region(&tickle_dev, 0, 1, "ondemandtcl");
+	if (res < 0) {
+		printk(KERN_WARNING "%s: can't alloc major number (%d)\n", __FILE__, res);
+		goto error;
+	}
+
+	tickle_class = class_create(THIS_MODULE, "ondemandtcl");
+	if (IS_ERR(tickle_class)) {
+		res = PTR_ERR(tickle_class);
+		printk(KERN_WARNING "%s: can't create class (%d)\n", __FILE__, res);
+		goto error_unregister_region;
+	}
+
+	cdev_init(&tickle_cdev, &tickle_fops);
+	tickle_cdev.owner = THIS_MODULE;
+
+	res = cdev_add(&tickle_cdev, tickle_dev, 1);
+	if (res < 0) {
+		printk(KERN_WARNING "%s: failed to add device (%d)\n", __FILE__, res);
+		goto error_remove_class;
+	}
+
+	/* always use ondemandtcl0, since userspace is already depending on that name */
+	dev = device_create(tickle_class, NULL, tickle_dev, "ondemandtcl%d", 0);
+	if (IS_ERR(dev)) {
+		res = PTR_ERR(dev);
+		printk(KERN_WARNING "%s: failed to create device (%d)\n", __FILE__, res);
+
+		goto error_remove_cdev;
+	}
+
+	/* add the default file data for anonymous clients */
+	default_file_data.floor_freq = 0;
+	list_add(&default_file_data.list, &tickle_clients);
+
+	return res;
+
+error_remove_cdev:
+	cdev_del(&tickle_cdev);
+
+error_remove_class:
+	class_destroy(tickle_class);
+	tickle_class = NULL;
+
+error_unregister_region:
+	unregister_chrdev_region(tickle_dev, 1);
+
+error:
+	return res;
+#endif
+}
+
+static void remove_tickle_device(void)
+{
+	if (tickle_class) {
+		device_destroy(tickle_class, tickle_dev);
+		class_destroy(tickle_class);
+		cdev_del(&tickle_cdev);
+		unregister_chrdev_region(tickle_dev, 1);
+
+		/* for posterity */
+		list_del(&default_file_data.list);
+	}
+}
+
+static int tickle_open(struct inode *inode, struct file *filp)
+{
+	int flags;
+	struct tickle_file_data *data = kzalloc(sizeof(struct tickle_file_data), GFP_KERNEL);
+	if (!data)
+		return -ENOMEM;
+
+	//printk("%s\n", __FUNCTION__);
+
+	spin_lock_irqsave(&tickle_state.lock, flags);
+	list_add(&data->list, &tickle_clients);
+	spin_unlock_irqrestore(&tickle_state.lock, flags);
+
+	filp->private_data = data;
+
+	return 0;
+}
+
+static int tickle_release(struct inode *inode, struct file *filp)
+{
+	int flags;
+	struct tickle_file_data *data = filp->private_data;
+
+	//printk("%s\n", __FUNCTION__);
+
+	if (data) {
+		spin_lock_irqsave(&tickle_state.lock, flags);
+		list_del(&data->list);
+		spin_unlock_irqrestore(&tickle_state.lock, flags);
+
+		CPUFREQ_UNHOLD_CHECK(&data->tickle_hold_flag);
+		CPUFREQ_FLOOR_UNHOLD_CHECK(&data->floor_hold_flag);
+
+		kfree(data);
+	}
+
+	return 0;
+}
+
+static int tickle_ioctl(struct inode *inode, struct file *filp, unsigned int cmd, unsigned long arg)
+{
+	int flags;
+	struct tickle_file_data *data = filp->private_data;
+
+	//printk("%s: cmd = %d, arg = %ld\n", __FUNCTION__, cmd, arg);
+
+	if (_IOC_TYPE(cmd) != TICKLE_IOC_MAGIC)
+		return -ENOTTY;
+
+	if (_IOC_NR(cmd) > TICKLE_IOC_MAXNR)
+		return -ENOTTY;
+
+	switch (cmd) {
+		case TICKLE_IOCT_TICKLE:
+			CPUFREQ_TICKLE_MILLIS((unsigned int) arg);
+			break;
+
+		case TICKLE_IOCT_FLOOR:
+			CPUFREQ_FLOOR((unsigned int) arg);
+			break;
+
+		case TICKLE_IOC_TICKLE_HOLD:
+			CPUFREQ_HOLD_CHECK(&data->floor_hold_flag);
+			break;
+
+		case TICKLE_IOC_TICKLE_UNHOLD:
+			CPUFREQ_UNHOLD_CHECK(&data->floor_hold_flag);
+			break;
+
+		case TICKLE_IOCT_FLOOR_HOLD:
+			spin_lock_irqsave(&tickle_state.lock, flags);
+			data->floor_freq = (unsigned int) arg;
+			spin_unlock_irqrestore(&tickle_state.lock, flags);
+
+			CPUFREQ_FLOOR_HOLD_CHECK((unsigned int) arg, &data->floor_hold_flag);
+			break;
+
+		case TICKLE_IOC_FLOOR_UNHOLD:
+			spin_lock_irqsave(&tickle_state.lock, flags);
+			data->floor_freq = 0;
+			spin_unlock_irqrestore(&tickle_state.lock, flags);
+
+			CPUFREQ_FLOOR_UNHOLD_CHECK(&data->floor_hold_flag);
+			break;
+
+		default:
+			return -ENOTTY;
+	}
+
+	return 0;
+}
+
+/* This needs to be called with spinlock tickle_state.lock held */
+static int get_max_client_floor(void)
+{
+	int floor_freq = 0;
+	struct tickle_file_data *file_data;
+
+	list_for_each_entry(file_data, &tickle_clients, list) {
+		if (file_data->floor_freq > floor_freq)
+			floor_freq = file_data->floor_freq;
+	}
+
+	return floor_freq;
+}
+
+/******************** end ioctl *********************/
+
+static inline cputime64_t get_cpu_idle_time(unsigned int cpu)
+{
+	cputime64_t idle_time;
+	cputime64_t cur_jiffies;
+	cputime64_t busy_time;
+
+	cur_jiffies = jiffies64_to_cputime64(get_jiffies_64());
+	busy_time = cputime64_add(kstat_cpu(cpu).cpustat.user,
+			kstat_cpu(cpu).cpustat.system);
+
+	busy_time = cputime64_add(busy_time, kstat_cpu(cpu).cpustat.irq);
+	busy_time = cputime64_add(busy_time, kstat_cpu(cpu).cpustat.softirq);
+	busy_time = cputime64_add(busy_time, kstat_cpu(cpu).cpustat.steal);
+
+	if (!dbs_tuners_ins.ignore_nice) {
+		busy_time = cputime64_add(busy_time,
+				kstat_cpu(cpu).cpustat.nice);
+	}
+
+	idle_time = cputime64_sub(cur_jiffies, busy_time);
+	return idle_time;
+}
+
+/*
+ * Find right freq to be set now with powersave_bias on.
+ * Returns the freq_hi to be used right now and will set freq_hi_jiffies,
+ * freq_lo, and freq_lo_jiffies in percpu area for averaging freqs.
+ */
+static unsigned int powersave_bias_target(struct cpufreq_policy *policy,
+					  unsigned int freq_next,
+					  unsigned int relation)
+{
+	unsigned int freq_req, freq_reduc, freq_avg;
+	unsigned int freq_hi, freq_lo;
+	unsigned int index = 0;
+	unsigned int jiffies_total, jiffies_hi, jiffies_lo;
+	struct cpu_dbs_info_s *dbs_info = &per_cpu(cpu_dbs_info, policy->cpu);
+
+	if (!dbs_info->freq_table) {
+		dbs_info->freq_lo = 0;
+		dbs_info->freq_lo_jiffies = 0;
+		return freq_next;
+	}
+
+	cpufreq_frequency_table_target(policy, dbs_info->freq_table, freq_next,
+			relation, &index);
+	freq_req = dbs_info->freq_table[index].frequency;
+	freq_reduc = freq_req * dbs_tuners_ins.powersave_bias / 1000;
+	freq_avg = freq_req - freq_reduc;
+
+	/* Find freq bounds for freq_avg in freq_table */
+	index = 0;
+	cpufreq_frequency_table_target(policy, dbs_info->freq_table, freq_avg,
+			CPUFREQ_RELATION_H, &index);
+	freq_lo = dbs_info->freq_table[index].frequency;
+	index = 0;
+	cpufreq_frequency_table_target(policy, dbs_info->freq_table, freq_avg,
+			CPUFREQ_RELATION_L, &index);
+	freq_hi = dbs_info->freq_table[index].frequency;
+
+	/* Find out how long we have to be in hi and lo freqs */
+	if (freq_hi == freq_lo) {
+		dbs_info->freq_lo = 0;
+		dbs_info->freq_lo_jiffies = 0;
+		return freq_lo;
+	}
+	jiffies_total = usecs_to_jiffies(dbs_tuners_ins.sampling_rate);
+	jiffies_hi = (freq_avg - freq_lo) * jiffies_total;
+	jiffies_hi += ((freq_hi - freq_lo) / 2);
+	jiffies_hi /= (freq_hi - freq_lo);
+	jiffies_lo = jiffies_total - jiffies_hi;
+	dbs_info->freq_lo = freq_lo;
+	dbs_info->freq_lo_jiffies = jiffies_lo;
+	dbs_info->freq_hi_jiffies = jiffies_hi;
+	return freq_hi;
+}
+
+static void ondemand_powersave_bias_init(void)
+{
+	int i;
+	for_each_online_cpu(i) {
+		struct cpu_dbs_info_s *dbs_info = &per_cpu(cpu_dbs_info, i);
+		dbs_info->freq_table = cpufreq_frequency_get_table(i);
+		dbs_info->freq_lo = 0;
+	}
+}
+
+/************************** sysfs interface ************************/
+static ssize_t show_sampling_rate_max(struct cpufreq_policy *policy, char *buf)
+{
+	return sprintf (buf, "%u\n", MAX_SAMPLING_RATE);
+}
+
+static ssize_t show_sampling_rate_min(struct cpufreq_policy *policy, char *buf)
+{
+	return sprintf (buf, "%u\n", MIN_SAMPLING_RATE);
+}
+
+#define define_one_ro(_name)		\
+static struct freq_attr _name =		\
+__ATTR(_name, 0444, show_##_name, NULL)
+
+define_one_ro(sampling_rate_max);
+define_one_ro(sampling_rate_min);
+
+/* cpufreq_ondemand Governor Tunables */
+#define show_one(file_name, object)					\
+static ssize_t show_##file_name						\
+(struct cpufreq_policy *unused, char *buf)				\
+{									\
+	return sprintf(buf, "%u\n", dbs_tuners_ins.object);		\
+}
+show_one(sampling_rate, sampling_rate);
+show_one(up_threshold, up_threshold);
+show_one(ignore_nice_load, ignore_nice);
+show_one(powersave_bias, powersave_bias);
+show_one(max_tickle_window, max_tickle_window);
+show_one(max_floor_window, max_floor_window);
+
+static ssize_t store_sampling_rate(struct cpufreq_policy *unused,
+		const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	mutex_lock(&dbs_mutex);
+	if (ret != 1 || input > MAX_SAMPLING_RATE
+		     || input < MIN_SAMPLING_RATE) {
+		mutex_unlock(&dbs_mutex);
+		return -EINVAL;
+	}
+
+	dbs_tuners_ins.sampling_rate = input;
+	mutex_unlock(&dbs_mutex);
+
+	return count;
+}
+
+static ssize_t store_up_threshold(struct cpufreq_policy *unused,
+		const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	mutex_lock(&dbs_mutex);
+	if (ret != 1 || input > MAX_FREQUENCY_UP_THRESHOLD ||
+			input < MIN_FREQUENCY_UP_THRESHOLD) {
+		mutex_unlock(&dbs_mutex);
+		return -EINVAL;
+	}
+
+	dbs_tuners_ins.up_threshold = input;
+	mutex_unlock(&dbs_mutex);
+
+	return count;
+}
+
+static ssize_t store_ignore_nice_load(struct cpufreq_policy *policy,
+		const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	unsigned int j;
+
+	ret = sscanf(buf, "%u", &input);
+	if ( ret != 1 )
+		return -EINVAL;
+
+	if ( input > 1 )
+		input = 1;
+
+	mutex_lock(&dbs_mutex);
+	if ( input == dbs_tuners_ins.ignore_nice ) { /* nothing to do */
+		mutex_unlock(&dbs_mutex);
+		return count;
+	}
+	dbs_tuners_ins.ignore_nice = input;
+
+	/* we need to re-evaluate prev_cpu_idle */
+	for_each_online_cpu(j) {
+		struct cpu_dbs_info_s *dbs_info;
+		dbs_info = &per_cpu(cpu_dbs_info, j);
+		dbs_info->prev_cpu_idle = get_cpu_idle_time(j);
+		dbs_info->prev_cpu_wall = get_jiffies_64();
+	}
+	mutex_unlock(&dbs_mutex);
+
+	return count;
+}
+
+static ssize_t store_powersave_bias(struct cpufreq_policy *unused,
+		const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1)
+		return -EINVAL;
+
+	if (input > 1000)
+		input = 1000;
+
+	mutex_lock(&dbs_mutex);
+	dbs_tuners_ins.powersave_bias = input;
+	ondemand_powersave_bias_init();
+	mutex_unlock(&dbs_mutex);
+
+	return count;
+}
+
+static ssize_t store_max_tickle_window(struct cpufreq_policy *unuesd,
+		const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1)
+		return -EINVAL;
+
+	if (input > MAX_TICKLE_WINDOW)
+		input = MAX_TICKLE_WINDOW;
+
+	mutex_lock(&dbs_mutex);
+	dbs_tuners_ins.max_tickle_window = input;
+	mutex_unlock(&dbs_mutex);
+
+	return count;
+}
+
+static ssize_t store_max_floor_window(struct cpufreq_policy *unuesd,
+		const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1)
+		return -EINVAL;
+
+	if (input > MAX_FLOOR_WINDOW)
+		input = MAX_FLOOR_WINDOW;
+
+	mutex_lock(&dbs_mutex);
+	dbs_tuners_ins.max_floor_window = input;
+	mutex_unlock(&dbs_mutex);
+
+	return count;
+}
+
+#define define_one_rw(_name) \
+static struct freq_attr _name = \
+__ATTR(_name, 0644, show_##_name, store_##_name)
+
+define_one_rw(sampling_rate);
+define_one_rw(up_threshold);
+define_one_rw(ignore_nice_load);
+define_one_rw(powersave_bias);
+define_one_rw(max_tickle_window);
+define_one_rw(max_floor_window);
+
+static struct attribute * dbs_attributes[] = {
+	&sampling_rate_max.attr,
+	&sampling_rate_min.attr,
+	&sampling_rate.attr,
+	&up_threshold.attr,
+	&ignore_nice_load.attr,
+	&powersave_bias.attr,
+	&max_tickle_window.attr,
+	&max_floor_window.attr,
+	NULL
+};
+
+static struct attribute_group dbs_attr_group = {
+	.attrs = dbs_attributes,
+	.name = "ondemandtcl",
+};
+
+/************************** sysfs end ************************/
+
+static void record_sample(unsigned int cur_freq, unsigned int target_freq,
+		int load, int cpu)
+{
+	struct sample_stats *stats;
+	cputime64_t cur_jiffies;
+	int i;
+
+	/* abuse a module parameter to trigger clearing the sample buffer */
+	if (clear_samples) {
+		for_each_online_cpu(i) {
+			stats = &per_cpu(sample_stats, i);
+			if (stats->samples)
+				memset(stats->samples, 0, sizeof(struct sample_data) * NUM_SAMPLES);
+			stats->current_sample = 0;
+		}
+		clear_samples = 0;
+	}
+
+	if (!sampling_enabled)
+		return;
+
+	cur_jiffies = jiffies64_to_cputime64(get_jiffies_64());
+	stats = &per_cpu(sample_stats, cpu);
+
+	if (!stats->samples)
+		return;
+
+	//if (target_freq != cur_freq) {
+		i = stats->current_sample;
+		stats->samples[i].timestamp = cur_jiffies;
+		stats->samples[i].user = kstat_cpu(cpu).cpustat.user;
+		stats->samples[i].system = kstat_cpu(cpu).cpustat.system;
+		stats->samples[i].irq = kstat_cpu(cpu).cpustat.irq;
+		stats->samples[i].softirq = kstat_cpu(cpu).cpustat.softirq;
+		stats->samples[i].steal = kstat_cpu(cpu).cpustat.steal;
+		stats->samples[i].nice = kstat_cpu(cpu).cpustat.nice;
+		stats->samples[i].cur_freq = cur_freq;
+		stats->samples[i].target_freq = target_freq;
+		stats->samples[i].load = load;
+		stats->current_sample = (i + 1) % NUM_SAMPLES;
+	//}
+}
+
+static void do_tickle_state_change(struct work_struct *work)
+{
+	int j, flags;
+	int active, active_tickle, active_count;
+	unsigned long tickle_jiffies;
+
+	spin_lock_irqsave(&tickle_state.lock, flags);
+
+	active = tickle_state.active;
+	active_count = tickle_state.active_count;
+	active_tickle = tickle_state.active_tickle;
+	tickle_jiffies = tickle_state.tickle_jiffies;
+
+	spin_unlock_irqrestore(&tickle_state.lock, flags);
+
+	if (print_tickles)
+		printk("%s: active=%u, active_count=%u, tickle_jiffies=%lu\n",
+				__FUNCTION__, active, active_count, tickle_jiffies);
+
+	if (active_count) {
+		if (!active) {
+			mutex_lock(&dbs_mutex);
+
+			for_each_online_cpu(j) {
+				struct cpu_dbs_info_s *dbs_info = &per_cpu(cpu_dbs_info, j);
+				struct cpufreq_policy *policy = dbs_info->cur_policy;
+
+				/* if we don't have a policy then we probably got tickled before setup completed */
+				if (!policy || !dbs_info->enable)
+					continue;
+
+				/* save the current operating frequency */
+				dbs_info->freq_save = policy->cur;
+
+				/* ramp up to the policy max */
+				if (policy->cur < policy->max) {
+					record_sample(policy->cur, policy->max, -2, policy->cpu);
+					cpufreq_driver_target(policy, policy->max, CPUFREQ_RELATION_H);
+				}
+
+				dbs_info->tickle_active = 1;
+			}
+
+			mutex_unlock(&dbs_mutex);
+		}
+
+		if (active_tickle)
+			mod_timer(&tickle_state.tickle_timer, tickle_jiffies);
+
+		active = 1;
+	} else if (active) {
+		mutex_lock(&dbs_mutex);
+
+		for_each_online_cpu(j) {
+			struct cpu_dbs_info_s *dbs_info = &per_cpu(cpu_dbs_info, j);
+			int cpu = dbs_info->cpu;
+
+			if (lock_policy_rwsem_write(cpu) < 0)
+				continue;
+
+			if (!dbs_info->enable) {
+				unlock_policy_rwsem_write(cpu);
+				continue;
+			}
+
+			dbs_info->tickle_active = 0;
+			adjust_for_load(dbs_info);
+
+			unlock_policy_rwsem_write(cpu);
+		}
+
+		mutex_unlock(&dbs_mutex);
+
+		active = 0;
+	}
+
+	/* update tickle_state */
+	spin_lock_irqsave(&tickle_state.lock, flags);
+	tickle_state.active = active;
+	spin_unlock_irqrestore(&tickle_state.lock, flags);
+}
+
+void cpufreq_ondemand_tickle_millis(unsigned int millis)
+{
+	int flags, queue = 0;
+	unsigned long expire;
+
+	if (!tickling_enabled)
+		return;
+
+	if (millis > dbs_tuners_ins.max_tickle_window)
+		millis = dbs_tuners_ins.max_tickle_window;
+
+	expire = jiffies + msecs_to_jiffies(millis);
+
+	spin_lock_irqsave(&tickle_state.lock, flags);
+
+	if (time_after(expire, tickle_state.tickle_jiffies)) {
+		tickle_state.tickle_jiffies = expire;
+
+		if (!tickle_state.active_tickle)
+			tickle_state.active_count += 1;
+
+		tickle_state.active_tickle = 1;
+		queue = 1;
+	}
+
+	spin_unlock_irqrestore(&tickle_state.lock, flags);
+
+	if (queue)
+		queue_work(kondemand_wq, &tickle_state.tickle_work);
+}
+EXPORT_SYMBOL(cpufreq_ondemand_tickle_millis);
+
+void cpufreq_ondemand_tickle(void)
+{
+	cpufreq_ondemand_tickle_millis(dbs_tuners_ins.max_tickle_window);
+}
+EXPORT_SYMBOL(cpufreq_ondemand_tickle);
+
+void cpufreq_ondemand_hold(void)
+{
+	int flags;
+
+	spin_lock_irqsave(&tickle_state.lock, flags);
+	tickle_state.active_count += 1;
+	spin_unlock_irqrestore(&tickle_state.lock, flags);
+
+	queue_work(kondemand_wq, &tickle_state.tickle_work);
+}
+EXPORT_SYMBOL(cpufreq_ondemand_hold);
+
+void cpufreq_ondemand_unhold(void)
+{
+	int flags, queue = 0;
+
+	spin_lock_irqsave(&tickle_state.lock, flags);
+
+	if (tickle_state.active_count) {
+		tickle_state.active_count -= 1;
+		queue = 1;
+	} else {
+		printk(KERN_WARNING "%s: attempt to decrement when active_count == 0!\n",
+				__FUNCTION__);
+	}
+
+	spin_unlock_irqrestore(&tickle_state.lock, flags);
+
+	if (queue)
+		queue_work(kondemand_wq, &tickle_state.tickle_work);
+}
+EXPORT_SYMBOL(cpufreq_ondemand_unhold);
+
+void cpufreq_ondemand_hold_check(int *flag)
+{
+	if (!*flag) {
+		cpufreq_ondemand_hold();
+		*flag = 1;
+	}
+}
+EXPORT_SYMBOL(cpufreq_ondemand_hold_check);
+
+void cpufreq_ondemand_unhold_check(int *flag)
+{
+	if (*flag) {
+		cpufreq_ondemand_unhold();
+		*flag = 0;
+	}
+}
+EXPORT_SYMBOL(cpufreq_ondemand_unhold_check);
+
+static void do_floor_state_change(struct work_struct *work)
+{
+	int j, flags, floor_active, floor_tickle, floor_count, cur_freq_floor, pending_freq_floor;
+	unsigned long floor_jiffies;
+
+	spin_lock_irqsave(&tickle_state.lock, flags);
+
+	floor_active = tickle_state.floor_active;
+	floor_tickle = tickle_state.floor_tickle;
+	floor_count = tickle_state.floor_count;
+	cur_freq_floor = tickle_state.cur_freq_floor;
+	floor_jiffies = tickle_state.floor_jiffies;
+
+	pending_freq_floor = get_max_client_floor();
+
+	spin_unlock_irqrestore(&tickle_state.lock, flags);
+
+
+	if (print_tickles)
+		printk("%s: floor_active=%u, floor_tickle=%u, floor_count=%u, floor_jiffies=%lu, "
+				"cur_freq_floor=%u, pending_freq_floor=%u\n",
+				__FUNCTION__, floor_active, floor_tickle, floor_count, floor_jiffies,
+				cur_freq_floor, pending_freq_floor);
+
+	if (floor_count) {
+		if (!floor_active || pending_freq_floor != cur_freq_floor) {
+			mutex_lock(&dbs_mutex);
+
+			for_each_online_cpu(j) {
+				unsigned int f = pending_freq_floor;
+				struct cpu_dbs_info_s *dbs_info = &per_cpu(cpu_dbs_info, j);
+				struct cpufreq_policy *policy = dbs_info->cur_policy;
+
+				if (!policy || !dbs_info->enable)
+					continue;
+
+				if (policy->min >= f)
+					continue;
+
+				if (f > policy->max)
+					f = policy->max;
+
+				dbs_info->freq_floor = f;
+				dbs_info->freq_save = policy->cur;
+				dbs_info->floor_active = 1;
+
+				if (policy->cur < f) {
+					record_sample(policy->cur, f, -3, policy->cpu);
+					cpufreq_driver_target(policy, f, CPUFREQ_RELATION_H);
+				}
+			}
+
+			mutex_unlock(&dbs_mutex);
+		}
+
+		if (floor_tickle)
+			mod_timer(&tickle_state.floor_timer, floor_jiffies);
+
+		floor_active = 1;
+		cur_freq_floor = pending_freq_floor;
+	} else if (floor_active) {
+		mutex_lock(&dbs_mutex);
+
+		for_each_online_cpu(j) {
+			struct cpu_dbs_info_s *dbs_info = &per_cpu(cpu_dbs_info, j);
+			struct cpufreq_policy *policy = dbs_info->cur_policy;
+			int cpu = dbs_info->cpu;
+
+			if (lock_policy_rwsem_write(cpu) < 0)
+				continue;
+
+			if (!dbs_info->enable) {
+				unlock_policy_rwsem_write(cpu);
+				continue;
+			}
+
+			dbs_info->floor_active = 0;
+			__cpufreq_driver_target(policy, dbs_info->freq_save, dbs_info->rel_save);
+
+			unlock_policy_rwsem_write(cpu);
+		}
+
+		mutex_unlock(&dbs_mutex);
+
+		floor_active = 0;
+		cur_freq_floor = 0;
+	}
+
+	spin_lock_irqsave(&tickle_state.lock, flags);
+	tickle_state.floor_active = floor_active;
+	tickle_state.cur_freq_floor = cur_freq_floor;
+	spin_unlock_irqrestore(&tickle_state.lock, flags);
+}
+
+void cpufreq_ondemand_floor_millis(unsigned int freq, unsigned int millis)
+{
+	int flags, queue = 0;
+	unsigned long expire;
+
+	if (!tickling_enabled)
+		return;
+
+	if (millis > dbs_tuners_ins.max_floor_window)
+		millis = dbs_tuners_ins.max_floor_window;
+
+	expire = jiffies + msecs_to_jiffies(millis);
+
+	spin_lock_irqsave(&tickle_state.lock, flags);
+
+	if (time_after(expire, tickle_state.floor_jiffies) ||
+			(freq >  get_max_client_floor() && freq > tickle_state.cur_freq_floor)) {
+		if (time_after(expire, tickle_state.floor_jiffies))
+			tickle_state.floor_jiffies = expire;
+
+		if (!tickle_state.floor_tickle)
+			tickle_state.floor_count += 1;
+
+		tickle_state.floor_tickle = 1;
+
+		default_file_data.floor_freq = max(default_file_data.floor_freq, freq);
+
+		queue = 1;
+	}
+
+	spin_unlock_irqrestore(&tickle_state.lock, flags);
+
+	if (queue)
+		queue_work(kondemand_wq, &tickle_state.floor_work);
+}
+EXPORT_SYMBOL(cpufreq_ondemand_floor_millis);
+
+void cpufreq_ondemand_floor(unsigned int freq)
+{
+	cpufreq_ondemand_floor_millis(freq, dbs_tuners_ins.max_floor_window);
+}
+EXPORT_SYMBOL(cpufreq_ondemand_floor);
+
+void cpufreq_ondemand_floor_hold(unsigned int freq)
+{
+	int flags;
+
+	spin_lock_irqsave(&tickle_state.lock, flags);
+	tickle_state.floor_count += 1;
+
+	default_file_data.floor_freq = freq;
+
+	spin_unlock_irqrestore(&tickle_state.lock, flags);
+
+	queue_work(kondemand_wq, &tickle_state.floor_work);
+}
+EXPORT_SYMBOL(cpufreq_ondemand_floor_hold);
+
+void cpufreq_ondemand_floor_unhold(void)
+{
+	int flags, queue = 0;
+
+	spin_lock_irqsave(&tickle_state.lock, flags);
+
+	if (tickle_state.floor_count) {
+		tickle_state.floor_count -= 1;
+		queue = 1;
+	} else {
+		printk(KERN_WARNING "%s: attempt to decrement when floor_count == 0!\n",
+				__FUNCTION__);
+	}
+
+	spin_unlock_irqrestore(&tickle_state.lock, flags);
+
+	if (queue)
+		queue_work(kondemand_wq, &tickle_state.floor_work);
+}
+EXPORT_SYMBOL(cpufreq_ondemand_floor_unhold);
+
+void cpufreq_ondemand_floor_hold_check(unsigned int freq, int *flag)
+{
+	if (!*flag) {
+		cpufreq_ondemand_floor_hold(freq);
+		*flag = 1;
+	}
+}
+EXPORT_SYMBOL(cpufreq_ondemand_floor_hold_check);
+
+void cpufreq_ondemand_floor_unhold_check(int *flag)
+{
+	if (*flag) {
+		cpufreq_ondemand_floor_unhold();
+		*flag = 0;
+	}
+}
+EXPORT_SYMBOL(cpufreq_ondemand_floor_unhold_check);
+
+
+static void dbs_check_cpu(struct cpu_dbs_info_s *this_dbs_info)
+{
+	unsigned int idle_ticks, total_ticks;
+	unsigned int load = 0;
+	cputime64_t cur_jiffies;
+
+	struct cpufreq_policy *policy;
+	unsigned int j;
+
+	this_dbs_info->cur_load = 0;
+
+	if (!this_dbs_info->enable)
+		return;
+
+	this_dbs_info->freq_lo = 0;
+	policy = this_dbs_info->cur_policy;
+	cur_jiffies = jiffies64_to_cputime64(get_jiffies_64());
+	total_ticks = (unsigned int) cputime64_sub(cur_jiffies,
+			this_dbs_info->prev_cpu_wall);
+	this_dbs_info->prev_cpu_wall = get_jiffies_64();
+
+	if (!total_ticks)
+		return;
+	/*
+	 * Every sampling_rate, we check, if current idle time is less
+	 * than 20% (default), then we try to increase frequency
+	 * Every sampling_rate, we look for a the lowest
+	 * frequency which can sustain the load while keeping idle time over
+	 * 30%. If such a frequency exist, we try to decrease to this frequency.
+	 *
+	 * Any frequency increase takes it to the maximum frequency.
+	 * Frequency reduction happens at minimum steps of
+	 * 5% (default) of current frequency
+	 */
+
+	/* Get Idle Time */
+	idle_ticks = UINT_MAX;
+	for_each_cpu_mask(j, policy->cpus) {
+		cputime64_t total_idle_ticks;
+		unsigned int tmp_idle_ticks;
+		struct cpu_dbs_info_s *j_dbs_info;
+
+		j_dbs_info = &per_cpu(cpu_dbs_info, j);
+		total_idle_ticks = get_cpu_idle_time(j);
+		tmp_idle_ticks = (unsigned int) cputime64_sub(total_idle_ticks,
+				j_dbs_info->prev_cpu_idle);
+		j_dbs_info->prev_cpu_idle = total_idle_ticks;
+
+		if (tmp_idle_ticks < idle_ticks)
+			idle_ticks = tmp_idle_ticks;
+	}
+
+	if (likely(total_ticks > idle_ticks))
+		load = (100 * (total_ticks - idle_ticks)) / total_ticks;
+
+	this_dbs_info->cur_load = load;
+}
+
+static void adjust_for_load(struct cpu_dbs_info_s *this_dbs_info)
+{
+	int load;
+	struct cpufreq_policy *policy;
+
+	if (!this_dbs_info->enable || this_dbs_info->tickle_active)
+		return;
+
+	policy = this_dbs_info->cur_policy;
+	load = this_dbs_info->cur_load;
+
+	/* Check for frequency increase */
+	if (load > dbs_tuners_ins.up_threshold) {
+		/* if we are already at full speed then break out early */
+		if (!dbs_tuners_ins.powersave_bias) {
+			record_sample(policy->cur, policy->max, load, policy->cpu);
+			if (policy->cur == policy->max)
+				return;
+
+			if (this_dbs_info->floor_active) {
+				this_dbs_info->freq_save = policy->max;
+				this_dbs_info->rel_save = CPUFREQ_RELATION_H;
+			}
+
+			__cpufreq_driver_target(policy, policy->max,
+				CPUFREQ_RELATION_H);
+		} else {
+			int freq = powersave_bias_target(policy, policy->max,
+					CPUFREQ_RELATION_H);
+
+			if (this_dbs_info->floor_active) {
+				this_dbs_info->freq_save = freq;
+				this_dbs_info->rel_save = CPUFREQ_RELATION_L;
+
+				if (freq <= this_dbs_info->freq_floor)
+					freq = this_dbs_info->freq_floor;
+			}
+
+			record_sample(policy->cur, freq, load, policy->cpu);
+			__cpufreq_driver_target(policy, freq,
+				CPUFREQ_RELATION_L);
+		}
+		return;
+	}
+
+	/* Check for frequency decrease */
+	/* if we cannot reduce the frequency anymore, break out early */
+	if (policy->cur == policy->min)
+		return;
+
+	/*
+	 * The optimal frequency is the frequency that is the lowest that
+	 * can support the current CPU usage without triggering the up
+	 * policy. To be safe, we focus 10 points under the threshold.
+	 */
+	if (load < (dbs_tuners_ins.up_threshold - 10)) {
+		unsigned int freq_next, freq_cur;
+
+		freq_cur = __cpufreq_driver_getavg(policy);
+		if (!freq_cur)
+			freq_cur = policy->cur;
+
+		freq_next = (freq_cur * load) /
+			(dbs_tuners_ins.up_threshold - 10);
+
+
+		if (!dbs_tuners_ins.powersave_bias) {
+			if (this_dbs_info->floor_active) {
+				this_dbs_info->freq_save = freq_next;
+				this_dbs_info->rel_save = CPUFREQ_RELATION_L;
+
+				if (freq_next <= this_dbs_info->freq_floor)
+					freq_next = this_dbs_info->freq_floor;
+			}
+
+			record_sample(policy->cur, freq_next, load, policy->cpu);
+			__cpufreq_driver_target(policy, freq_next,
+					CPUFREQ_RELATION_L);
+		} else {
+			int freq = powersave_bias_target(policy, freq_next,
+					CPUFREQ_RELATION_L);
+
+			if (this_dbs_info->floor_active) {
+				this_dbs_info->freq_save = freq;
+				this_dbs_info->rel_save = CPUFREQ_RELATION_L;
+
+				if (freq <= this_dbs_info->freq_floor)
+					freq = this_dbs_info->freq_floor;
+			}
+
+			__cpufreq_driver_target(policy, freq,
+				CPUFREQ_RELATION_L);
+			record_sample(policy->cur, freq, load, policy->cpu);
+		}
+	}
+}
+
+static void do_dbs_timer(struct work_struct *work)
+{
+	struct cpu_dbs_info_s *dbs_info =
+		container_of(work, struct cpu_dbs_info_s, work.work);
+	unsigned int cpu = dbs_info->cpu;
+	int sample_type = dbs_info->sample_type;
+
+	/* We want all CPUs to do sampling nearly on same jiffy */
+	int delay = usecs_to_jiffies(dbs_tuners_ins.sampling_rate);
+
+	delay -= jiffies % delay;
+
+	if (lock_policy_rwsem_write(cpu) < 0)
+		return;
+
+	if (!dbs_info->enable) {
+		unlock_policy_rwsem_write(cpu);
+		return;
+	}
+
+	/* Common NORMAL_SAMPLE setup */
+	dbs_info->sample_type = DBS_NORMAL_SAMPLE;
+	if (!dbs_tuners_ins.powersave_bias ||
+	    sample_type == DBS_NORMAL_SAMPLE) {
+		dbs_check_cpu(dbs_info);
+		adjust_for_load(dbs_info);
+
+		if (dbs_info->freq_lo) {
+			/* Setup timer for SUB_SAMPLE */
+			dbs_info->sample_type = DBS_SUB_SAMPLE;
+			delay = dbs_info->freq_hi_jiffies;
+		}
+	} else {
+		record_sample(dbs_info->cur_policy->cur, dbs_info->freq_lo, -1, cpu);
+		__cpufreq_driver_target(dbs_info->cur_policy,
+					dbs_info->freq_lo,
+					CPUFREQ_RELATION_H);
+	}
+	queue_delayed_work_on(cpu, kondemand_wq, &dbs_info->work, delay);
+	unlock_policy_rwsem_write(cpu);
+}
+
+static void do_tickle_timer(unsigned long arg)
+{
+	int flags;
+
+	spin_lock_irqsave(&tickle_state.lock, flags);
+
+	if (tickle_state.active_count) {
+		if (tickle_state.active_tickle) {
+			tickle_state.active_count -= 1;
+			tickle_state.active_tickle = 0;
+		}
+	} else {
+		printk(KERN_WARNING "%s: attempt to decrement when active_count == 0!\n",
+				__FUNCTION__);
+	}
+
+	spin_unlock_irqrestore(&tickle_state.lock, flags);
+
+	queue_work(kondemand_wq, &tickle_state.tickle_work);
+}
+
+static void do_floor_timer(unsigned long arg)
+{
+	int flags;
+
+	spin_lock_irqsave(&tickle_state.lock, flags);
+
+	if (tickle_state.floor_count) {
+		if (tickle_state.floor_tickle) {
+			tickle_state.floor_count -= 1;
+			tickle_state.floor_tickle = 0;
+		}
+	} else {
+		printk(KERN_WARNING "%s: attempt to decrement when floor_count == 0!\n",
+				__FUNCTION__);
+	}
+
+	spin_unlock_irqrestore(&tickle_state.lock, flags);
+
+	queue_work(kondemand_wq, &tickle_state.floor_work);
+}
+
+static inline void dbs_timer_init(struct cpu_dbs_info_s *dbs_info)
+{
+	/* We want all CPUs to do sampling nearly on same jiffy */
+	int delay = usecs_to_jiffies(dbs_tuners_ins.sampling_rate);
+	delay -= jiffies % delay;
+
+	dbs_info->enable = 1;
+	ondemand_powersave_bias_init();
+	dbs_info->sample_type = DBS_NORMAL_SAMPLE;
+	INIT_DELAYED_WORK_DEFERRABLE(&dbs_info->work, do_dbs_timer);
+	queue_delayed_work_on(dbs_info->cpu, kondemand_wq, &dbs_info->work,
+	                      delay);
+
+	dbs_info->tickle_active = 0;
+	dbs_info->floor_active = 0;
+}
+
+static inline void dbs_timer_exit(struct cpu_dbs_info_s *dbs_info)
+{
+	dbs_info->enable = 0;
+	cancel_delayed_work(&dbs_info->work);
+}
+
+static int cpufreq_governor_dbs(struct cpufreq_policy *policy,
+				   unsigned int event)
+{
+	unsigned int cpu = policy->cpu;
+	struct cpu_dbs_info_s *this_dbs_info;
+	unsigned int j;
+	int rc;
+	struct proc_dir_entry *entry;
+
+	this_dbs_info = &per_cpu(cpu_dbs_info, cpu);
+
+	switch (event) {
+	case CPUFREQ_GOV_START:
+		if ((!cpu_online(cpu)) || (!policy->cur))
+			return -EINVAL;
+
+		if (this_dbs_info->enable) /* Already enabled */
+			break;
+
+		mutex_lock(&dbs_mutex);
+		dbs_enable++;
+
+		rc = sysfs_create_group(&policy->kobj, &dbs_attr_group);
+		if (rc) {
+			dbs_enable--;
+			mutex_unlock(&dbs_mutex);
+			return rc;
+		}
+
+		for_each_cpu_mask(j, policy->cpus) {
+			struct cpu_dbs_info_s *j_dbs_info;
+			j_dbs_info = &per_cpu(cpu_dbs_info, j);
+			j_dbs_info->cur_policy = policy;
+
+			j_dbs_info->prev_cpu_idle = get_cpu_idle_time(j);
+			j_dbs_info->prev_cpu_wall = get_jiffies_64();
+		}
+		this_dbs_info->cpu = cpu;
+
+		/* sampling must be enabled prior to becoming the active governor */
+		if (sampling_enabled) {
+			for_each_cpu_mask(j, policy->cpus) {
+				struct sample_stats *stats = &per_cpu(sample_stats, j);
+
+				// if allocation fails, sampling is disabled on this cpu
+				if (!stats->samples)
+					stats->samples = vmalloc(sizeof(struct sample_data) * NUM_SAMPLES);
+
+				if (!stats->samples)
+					continue;
+
+				// touch each page to force allocation of physical pages
+				memset(stats->samples, 0, sizeof(struct sample_data) * NUM_SAMPLES);
+			}
+
+			entry = create_proc_entry("ondemandtcl_samples", 0444, NULL);
+			if (entry)
+				entry->proc_fops = &proc_stats_operations;
+		}
+
+		/*
+		 * Start the timerschedule work, when this governor
+		 * is used for first time
+		 */
+		if (dbs_enable == 1) {
+			unsigned int latency;
+			/* policy latency is in nS. Convert it to uS first */
+			latency = policy->cpuinfo.transition_latency / 1000;
+			if (latency == 0)
+				latency = 1;
+
+			def_sampling_rate = latency *
+					DEF_SAMPLING_RATE_LATENCY_MULTIPLIER;
+
+			if (def_sampling_rate < MIN_STAT_SAMPLING_RATE)
+				def_sampling_rate = MIN_STAT_SAMPLING_RATE;
+
+			dbs_tuners_ins.sampling_rate = def_sampling_rate;
+
+			init_timer(&tickle_state.tickle_timer);
+			init_timer(&tickle_state.floor_timer);
+
+			tickle_state.tickle_timer.function = do_tickle_timer;
+			tickle_state.floor_timer.function = do_floor_timer;
+
+			INIT_WORK(&tickle_state.tickle_work, do_tickle_state_change);
+			INIT_WORK(&tickle_state.floor_work, do_floor_state_change);
+
+			/* init these to current jiffies or short circuiting doesn't work until jiffies wraps */
+			tickle_state.tickle_jiffies = tickle_state.floor_jiffies = jiffies;
+		}
+		dbs_timer_init(this_dbs_info);
+
+		tickling_enabled = 1;
+
+		mutex_unlock(&dbs_mutex);
+		break;
+
+	case CPUFREQ_GOV_STOP:
+		mutex_lock(&dbs_mutex);
+
+		tickling_enabled = 0;
+		
+		dbs_timer_exit(this_dbs_info);
+		sysfs_remove_group(&policy->kobj, &dbs_attr_group);
+		dbs_enable--;
+
+		for_each_cpu_mask(j, policy->cpus) {
+			struct sample_stats *stats = &per_cpu(sample_stats, j);
+
+			if (stats->samples)
+				vfree(stats->samples);
+
+			stats->samples = NULL;
+		}
+
+		remove_proc_entry("ondemand_samples", NULL);
+
+		mutex_unlock(&dbs_mutex);
+
+		// TODO: cancel tickle work and timers
+		break;
+
+	case CPUFREQ_GOV_LIMITS:
+		mutex_lock(&dbs_mutex);
+		if (policy->max < this_dbs_info->cur_policy->cur) {
+			record_sample(policy->cur, policy->max, -1, policy->cpu);
+			__cpufreq_driver_target(this_dbs_info->cur_policy,
+			                        policy->max,
+			                        CPUFREQ_RELATION_H);
+		} else if (policy->min > this_dbs_info->cur_policy->cur) {
+			record_sample(policy->cur, policy->min, -1, policy->cpu);
+			__cpufreq_driver_target(this_dbs_info->cur_policy,
+			                        policy->min,
+			                        CPUFREQ_RELATION_L);
+		}
+		mutex_unlock(&dbs_mutex);
+		break;
+	}
+	return 0;
+}
+
+struct cpufreq_governor cpufreq_gov_ondemand_tickle = {
+	.name			= "ondemandtcl",
+	.governor		= cpufreq_governor_dbs,
+	.max_transition_latency = TRANSITION_LATENCY_LIMIT,
+	.owner			= THIS_MODULE,
+};
+EXPORT_SYMBOL(cpufreq_gov_ondemand_tickle);
+
+static int __init cpufreq_gov_dbs_init(void)
+{
+	int res;
+
+	kondemand_wq = create_workqueue("kondemandtcl");
+	if (!kondemand_wq) {
+		printk(KERN_ERR "Creation of kondemandtcl failed\n");
+		return -EFAULT;
+	}
+
+	res = setup_tickle_device();
+	if (res < 0)
+		return res;
+
+	return cpufreq_register_governor(&cpufreq_gov_ondemand_tickle);
+}
+
+static void __exit cpufreq_gov_dbs_exit(void)
+{
+	remove_tickle_device();
+	cpufreq_unregister_governor(&cpufreq_gov_ondemand_tickle);
+	destroy_workqueue(kondemand_wq);
+}
+
+
+MODULE_AUTHOR("Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>");
+MODULE_AUTHOR("Alexey Starikovskiy <alexey.y.starikovskiy@intel.com>");
+MODULE_AUTHOR("Corey Tabaka <corey.tabaka@palm.com>");
+MODULE_DESCRIPTION("'cpufreq_ondemand_tickle' - A dynamic cpufreq governor for "
+                   "Low Latency Frequency Transition capable processors");
+MODULE_LICENSE("GPL");
+
+#ifdef CONFIG_CPU_FREQ_DEFAULT_GOV_ONDEMAND_TICKLE
+fs_initcall(cpufreq_gov_dbs_init);
+#else
+module_init(cpufreq_gov_dbs_init);
+#endif
+module_exit(cpufreq_gov_dbs_exit);
--- linux-2.6.24-palm/include/linux/cpufreq_tickle.h	1969-12-31 19:00:00.000000000 -0500
+++ linux-2.6.24-ss/include/linux/cpufreq_tickle.h	2010-12-23 09:19:34.000000000 -0500
@@ -0,0 +1,31 @@
+/*
+ *  include/linux/cpufreq_tickle.h
+ *
+ *  Userspace interface to cpufreq_ondemand_tickle.
+ *
+ *  Copyright (C) 2009 Palm, Inc.
+ *                     Corey Tabaka <corey.tabaka@palm.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+#ifndef _LINUX_CPUFREQ_TICKLE_H
+#define _LINUX_CPUFREQ_TICKLE_H
+
+#include <linux/ioctl.h>
+
+/*
+ * The 'T' after TICKLE_IOC stands for "Tell" and denotes a direct argument value.
+ */
+#define TICKLE_IOC_MAGIC 'k'
+#define TICKLE_IOCT_TICKLE			_IO(TICKLE_IOC_MAGIC, 0)
+#define TICKLE_IOCT_FLOOR			_IO(TICKLE_IOC_MAGIC, 1)
+#define TICKLE_IOC_TICKLE_HOLD		_IO(TICKLE_IOC_MAGIC, 2)
+#define TICKLE_IOC_TICKLE_UNHOLD	_IO(TICKLE_IOC_MAGIC, 3)
+#define TICKLE_IOCT_FLOOR_HOLD		_IO(TICKLE_IOC_MAGIC, 4)
+#define TICKLE_IOC_FLOOR_UNHOLD		_IO(TICKLE_IOC_MAGIC, 5)
+
+#define TICKLE_IOC_MAXNR 5
+
+#endif /* _LINUX_CPUFREQ_TICKLE_H */

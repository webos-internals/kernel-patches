--- a/arch/arm/mach-msm/acpuclock-8x60.c	2011-08-02 16:04:21.000000000 -0400
+++ b/arch/arm/mach-msm/acpuclock-8x60.c	2011-09-11 12:10:47.000000000 -0400
@@ -394,7 +420,7 @@
 		return;
 	}
 
-	/* Update bandwidth if requst has changed. */
+	/* Update bandwidth if requst has changed. This may sleep. */
 	ret = msm_bus_scale_client_update_request(bus_perf_client, bw);
 	if (ret)
 		pr_err("%s: bandwidth request failed (%d)\n", __func__, ret);
@@ -404,7 +430,7 @@
 
 /* Apply any per-cpu voltage increases. */
 static int increase_vdd(int cpu, unsigned int vdd_sc, unsigned int vdd_mem,
-			unsigned int vdd_dig)
+			unsigned int vdd_dig, enum setrate_reason reason)
 {
 	int rc = 0;
 
@@ -427,6 +453,13 @@
 		return rc;
 	}
 
+	 /* Don't update the Scorpion voltage in the hotplug path.  It should
+	  * already be correct.  Attempting to set it is bad because we don't
+	  * know what CPU we are running on at this point, but the Scorpion
+	  * regulator API requires we call it from the affected CPU.  */
+	if (reason == SETRATE_HOTPLUG)
+		return rc;
+
 	/* Update per-core Scorpion voltage. */
 	rc = regulator_set_voltage(regulator_sc[cpu], vdd_sc, MAX_VDD_SC);
 	if (rc) {
@@ -440,16 +473,21 @@
 
 /* Apply any per-cpu voltage decreases. */
 static void decrease_vdd(int cpu, unsigned int vdd_sc, unsigned int vdd_mem,
-			 unsigned int vdd_dig)
+			 unsigned int vdd_dig, enum setrate_reason reason)
 {
 	int ret;
 
-	/* Update per-core Scorpion voltage. */
-	ret = regulator_set_voltage(regulator_sc[cpu], vdd_sc, MAX_VDD_SC);
-	if (ret) {
-		pr_err("%s: vdd_sc (cpu%d) decrease failed (%d)\n",
-			__func__, cpu, ret);
-		return;
+	/* Update per-core Scorpion voltage. This must be called on the CPU
+	 * that's being affected. Don't do this in the hotplug remove path,
+	 * where the rail is off and we're executing on the other CPU. */
+	if (reason != SETRATE_HOTPLUG) {
+		ret = regulator_set_voltage(regulator_sc[cpu], vdd_sc,
+					    MAX_VDD_SC);
+		if (ret) {
+			pr_err("%s: vdd_sc (cpu%d) decrease failed (%d)\n",
+				__func__, cpu, ret);
+			return;
+		}
 	}
 
 	/* Decrease vdd_dig active-set vote. */
@@ -510,7 +548,7 @@
 		goto out;
 	}
 
-	if (reason == SETRATE_CPUFREQ)
+	if (reason == SETRATE_CPUFREQ || reason == SETRATE_HOTPLUG)
 		mutex_lock(&drv_state.lock);
 
 	strt_s = drv_state.current_speed[cpu];
@@ -549,9 +587,10 @@
 	vdd_dig = max(tgt_s->l2_level->vdd_dig, pll_vdd_dig);
 
 	/* Increase VDD levels if needed. */
-	if ((reason == SETRATE_CPUFREQ || reason == SETRATE_INIT)
+	if ((reason == SETRATE_CPUFREQ || reason == SETRATE_HOTPLUG
+	  || reason == SETRATE_INIT)
 			&& (tgt_s->acpuclk_khz > strt_s->acpuclk_khz)) {
-		rc = increase_vdd(cpu, tgt_s->vdd_sc, vdd_mem, vdd_dig);
+		rc = increase_vdd(cpu, tgt_s->vdd_sc, vdd_mem, vdd_dig, reason);
 		if (rc)
 			goto out;
 	}
@@ -581,7 +620,7 @@
 
 	/* Drop VDD levels if we can. */
 	if (tgt_s->acpuclk_khz < strt_s->acpuclk_khz)
-		decrease_vdd(cpu, tgt_s->vdd_sc, vdd_mem, vdd_dig);
+		decrease_vdd(cpu, tgt_s->vdd_sc, vdd_mem, vdd_dig, reason);
 
 	dprintk("ACPU%d speed change complete\n", cpu);
 
@@ -590,7 +629,7 @@
 		AVS_ENABLE(cpu, tgt_s->avsdscr_setting);
 
 out:
-	if (reason == SETRATE_CPUFREQ)
+	if (reason == SETRATE_CPUFREQ || reason == SETRATE_HOTPLUG)
 		mutex_unlock(&drv_state.lock);
 	return rc;
 }
@@ -749,37 +788,38 @@
 static void __init cpufreq_table_init(void) {}
 #endif
 
-static unsigned int __init select_freq_plan(void)
-{
-	uint32_t speed_bin, max_khz;
-	struct clkctl_acpu_speed *f;
-
-	acpu_freq_tbl = acpu_freq_tbl_v2;
-	l2_freq_tbl = l2_freq_tbl_v2;
-	l2_freq_tbl_size = ARRAY_SIZE(l2_freq_tbl_v2);
-
-	speed_bin = readl(QFPROM_SPEED_BIN_PRI) & 0xF;
-	if (speed_bin == 0x1)
-		max_khz = 1512000;
-	else
-		max_khz = 1188000;
-
-	/* Truncate the table based to max_khz. */
-	for (f = acpu_freq_tbl; f->acpuclk_khz != 0; f++) {
-		if (f->acpuclk_khz > max_khz) {
-			f->acpuclk_khz = 0;
-			break;
-		}
+#define HOT_UNPLUG_KHZ MAX_AXI
+static int __cpuinit acpuclock_cpu_callback(struct notifier_block *nfb,
+					unsigned long action, void *hcpu)
+{
+	static int prev_khz[NR_CPUS];
+	int cpu = (int)hcpu;
+
+	switch (action) {
+	case CPU_DEAD:
+		  prev_khz[cpu] = acpuclk_get_rate(cpu);
+		  /* Fall through. */
+	case CPU_UP_CANCELED:
+		  acpuclk_set_rate(cpu, HOT_UNPLUG_KHZ, SETRATE_HOTPLUG);
+		  break;
+	case CPU_UP_PREPARE:
+		  if (WARN_ON(!prev_khz[cpu]))
+			    prev_khz[cpu] = acpu_freq_tbl->acpuclk_khz;
+		  acpuclk_set_rate(cpu, prev_khz[cpu], SETRATE_HOTPLUG);
+		  break;
+	default:
+		  break;
 	}
-	f--;
-	pr_info("Max ACPU freq: %u KHz\n", f->acpuclk_khz);
 
-	return f->acpuclk_khz;
+	return NOTIFY_OK;
 }
 
+static struct notifier_block __cpuinitdata acpuclock_cpu_notifier = {
+      .notifier_call = acpuclock_cpu_callback,
+};
+
 void __init msm_acpu_clock_init(struct msm_acpu_clock_platform_data *clkdata)
 {
-	unsigned int max_cpu_khz;
 	int cpu;
 
 	mutex_init(&drv_state.lock);
@@ -788,7 +828,10 @@
 	drv_state.vdd_switch_time_us = clkdata->vdd_switch_time_us;
 
 	/* Configure hardware. */
-	max_cpu_khz = select_freq_plan();
+	acpu_freq_tbl = acpu_freq_tbl_v2;
+	l2_freq_tbl = l2_freq_tbl_v2;
+	l2_freq_tbl_size = ARRAY_SIZE(l2_freq_tbl_v2);
+
 	unselect_scplls();
 	scpll_set_refs();
 	for_each_possible_cpu(cpu)
@@ -799,7 +842,8 @@
 
 	/* Improve boot time by ramping up CPUs immediately. */
 	for_each_online_cpu(cpu)
-		acpuclk_set_rate(cpu, max_cpu_khz, SETRATE_INIT);
+		acpuclk_set_rate(cpu, 1188000, SETRATE_INIT);
 
 	cpufreq_table_init();
+	register_hotcpu_notifier(&acpuclock_cpu_notifier);
 }
--- a/kernel/sched_fair.c	2011-08-02 16:04:22.000000000 -0400
+++ b/kernel/sched_fair.c	2011-09-11 11:30:23.000000000 -0400
@@ -3253,9 +3253,10 @@
 		cpu_rq(cpu)->in_nohz_recently = 1;
 
 		if (!cpu_active(cpu)) {
-			if (atomic_read(&nohz.load_balancer) != cpu)
+                      if (atomic_read(&nohz.load_balancer) != cpu) {
+				cpumask_clear_cpu(cpu, nohz.cpu_mask);
 				return 0;
-
+			}
 			/*
 			 * If we are going offline and still the leader,
 			 * give up!
@@ -3263,6 +3264,8 @@
 			if (atomic_cmpxchg(&nohz.load_balancer, cpu, -1) != cpu)
 				BUG();
 
+			cpumask_clear_cpu(cpu, nohz.cpu_mask);
+
 			return 0;
 		}
 
--- a/arch/arm/mm/cache-v7.S	2011-08-02 16:04:21.000000000 -0400
+++ b/arch/arm/mm/cache-v7.S	2011-09-10 22:40:16.000000000 -0400
@@ -161,15 +161,15 @@
  UNWIND(.fnstart		)
 	dcache_line_size r2, r3
 	sub	r3, r2, #1
-	bic	r0, r0, r3
+	bic	r12, r0, r3
 #ifdef CONFIG_SCORPION_Uni_45nm_BUG
 1:
- USER(	mcr	p15, 0, r0, c7, c11, 1	)	@ clean D line to PoU (DCCMVAU)
-	add	r0, r0, r2
-2:
-	cmp	r0, r1
+ USER(	mcr	p15, 0, r12, c7, c11, 1	)	@ clean D line to PoU (DCCMVAU)
+	add	r12, r12, r2
+	cmp	r12, r1
 	blo	1b
 	dsb
+3:
 	mov	r0, #0
 	mcr	p15, 0, r0, c7, c5, 0		@ invalidate I-Cache and BTB
 	dsb
@@ -177,13 +177,20 @@
 	mov	pc, lr
 #else
 1:
- USER(	mcr	p15, 0, r0, c7, c11, 1	)	@ clean D line to the point of unification
+ USER(	mcr	p15, 0, r12, c7, c11, 1	)	@ clean D line to the point of unification
+	add	r12, r12, r2
+	cmp	r12, r1
+	blo	1b
 	dsb
- USER(	mcr	p15, 0, r0, c7, c5, 1	)	@ invalidate I line
-	add	r0, r0, r2
+	icache_line_size r2, r3
+	sub	r3, r2, #1
+	bic	r12, r0, r3
 2:
-	cmp	r0, r1
-	blo	1b
+ USER(	mcr	p15, 0, r12, c7, c5, 1  )	@ invalidate I line
+	add	r12, r12, r2
+	cmp	r12, r1
+	blo	2b
+3:
 	mov	r0, #0
 #ifdef CONFIG_SMP
 	mcr	p15, 0, r0, c7, c1, 6		@ invalidate BTB Inner Shareable
@@ -200,10 +207,10 @@
  * isn't mapped, just try the next page.
  */
 9001:
-	mov	r0, r0, lsr #12
-	mov	r0, r0, lsl #12
-	add	r0, r0, #4096
-	b	2b
+	mov	r12, r12, lsr #12
+	mov	r12, r12, lsl #12
+	add	r12, r12, #4096
+	b	3b
  UNWIND(.fnend		)
 ENDPROC(v7_coherent_kern_range)
 ENDPROC(v7_coherent_user_range)
--- a/arch/arm/mm/proc-macros.S	2011-08-02 16:04:21.000000000 -0400
+++ b/arch/arm/mm/proc-macros.S	2011-09-10 22:40:40.000000000 -0400
@@ -72,6 +72,16 @@
 	mov	\reg, \reg, lsl \tmp		@ actual cache line size
 	.endm
 
+/*
+ * icache_line_size - get the minimum I-cache line size from the CTR register
+ * on ARMv7.
+ */
+	.macro	icache_line_size, reg, tmp
+	mrc	p15, 0, \tmp, c0, c0, 1		@ read ctr
+	and	\tmp, \tmp, #0xf		@ cache line size encoding
+	mov	\reg, #4			@ bytes per word
+	mov	\reg, \reg, lsl \tmp		@ actual cache line size
+	.endm
 
 /*
  * Sanity check the PTE configuration for the code below - which makes
--- a/arch/arm/mach-msm/pm-8x60.c	2011-08-02 16:04:21.000000000 -0400
+++ b/arch/arm/mach-msm/pm-8x60.c	2011-09-11 13:09:38.000000000 -0400
@@ -603,7 +603,6 @@
 struct msm_pm_device {
 	unsigned int cpu;
 #ifdef CONFIG_HOTPLUG_CPU
-	unsigned long saved_acpu_rate;
 	struct completion cpu_killed;
 	unsigned int warm_boot;
 #endif
@@ -724,7 +723,12 @@
 
 	avsdscr_setting = avs_get_avsdscr();
 	avs_disable();
-	saved_acpuclk_rate = acpuclk_power_collapse();
+
+	if (cpu_online(dev->cpu))
+		saved_acpuclk_rate = acpuclk_power_collapse();
+	else
+		saved_acpuclk_rate = 0;
+
 	if (MSM_PM_DEBUG_CLOCK & msm_pm_debug_mask)
 		pr_info("CPU%u: %s: change clock rate (old rate = %lu)\n",
 			dev->cpu, __func__, saved_acpuclk_rate);
@@ -1113,12 +1117,8 @@
 	flush_cache_all();
 
 	for (;;) {
-		if (allow[MSM_PM_SLEEP_MODE_POWER_COLLAPSE]) {
-			struct msm_pm_device *dev =
-				&__get_cpu_var(msm_pm_devices);
-			dev->saved_acpu_rate = acpuclk_get_rate(cpu);
+		if (allow[MSM_PM_SLEEP_MODE_POWER_COLLAPSE])
 			msm_pm_power_collapse(false);
-		}
 		else if (allow[MSM_PM_SLEEP_MODE_POWER_COLLAPSE_STANDALONE])
 			msm_pm_power_collapse_standalone(false);
 		else if (allow[MSM_PM_SLEEP_MODE_WAIT_FOR_INTERRUPT])
@@ -1147,15 +1147,6 @@
 	vfp_reinit();
 #endif
 
-	if (dev->saved_acpu_rate) {
-		ret = acpuclk_set_rate(dev->cpu,
-				dev->saved_acpu_rate,
-				SETRATE_PC);
-		if (ret)
-			pr_err("CPU%u: %s: failed clock rate restore(%lu)\n",
-			dev->cpu, __func__, dev->saved_acpu_rate);
-		dev->saved_acpu_rate = 0;
-	}
 	ret = msm_spm_set_low_power_mode(MSM_SPM_MODE_CLOCK_GATING, false);
 
 	return ret;
--- a/arch/arm/mach-msm/acpuclock.h	2011-08-02 16:04:21.000000000 -0400
+++ b/arch/arm/mach-msm/acpuclock.h	2011-09-11 12:11:10.000000000 -0400
@@ -26,6 +26,7 @@
 	SETRATE_CPUFREQ = 0,
 	SETRATE_SWFI,
 	SETRATE_PC,
+	SETRATE_HOTPLUG,
 	SETRATE_INIT,
 };
 
